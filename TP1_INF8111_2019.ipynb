{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "TP1_INF8111_2019.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FrOJucfg4pZ",
        "colab_type": "text"
      },
      "source": [
        "# INF8111 - Fouille de données\n",
        "\n",
        "\n",
        "## TP1 Automne 2019 - Preprocessing de tweets pour de l'analyse de sentiments\n",
        "\n",
        "##### Membres de l'équipe:\n",
        "\n",
        "    - Nom (Matricule) 1\n",
        "    - Nom (Matricule) 2\n",
        "    - Nom (Matricule) 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOrkwKxwg4pb",
        "colab_type": "text"
      },
      "source": [
        "## Présentation\n",
        "\n",
        "Twitter est un réseau social permettant aux utilisateurs de publier des informations et communiquer entre eux par le biais de messages, appelés tweets, pouvant contenir jusqu'à 280 caractères. Largement utilisé aujourd'hui, ce réseau peut être un outil pour des entreprises qui souhaitent évaluer l'avis de leurs clients.\n",
        "\n",
        "Dans ce TP, on se met à la place d'une compagnie aérienne, qui souhaiterait détecter les tweets qui la mentionnent et analyser si ce sont des mentions positives ou négatives, en comparant leur résultat avec les autres compagnies.\n",
        "\n",
        "Le *preprocessing* est une tâche cruciale en fouille de données. Elle permet de transformer les données brutes en un format adapté à l'application de méthodes de machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "r_2OPLwjg4pc",
        "colab_type": "text"
      },
      "source": [
        "# I/ Analyse de sentiments (13 Pts)\n",
        "\n",
        "Usuellement dans la littérature, la tâche d'extraire le sentiment d'un texte est appelé *sentiment analysis*.\n",
        "Ici pour se faire, nous allons utiliser un modèle *bag-of-words* (BoW).\n",
        "\n",
        "## 1. Installation\n",
        "\n",
        "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
        "\n",
        "Installez les libraires en question et exécutez le code ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoZsWXCkg4pd",
        "colab_type": "code",
        "outputId": "301e81a6-e447-47e6-8b9d-c7edcdfbaa02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v1hYCG6g4pg",
        "colab_type": "code",
        "outputId": "9e67ab86-7790-46fd-ce38-9c3eaefc866f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# If you want, you can use anaconda and install after nltk library\n",
        "# !pip install --user numpy\n",
        "# !pip install --user sklearn\n",
        "# !pip install --user scipy\n",
        "# !pip install --user nltk\n",
        "\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"universal_tagset\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhC1VWBgg4pj",
        "colab_type": "text"
      },
      "source": [
        "## 2. Jeu de données\n",
        "\n",
        "On utilise un jeu de donnée provenant de *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
        "\n",
        "Pour citer la source originale de la base :\n",
        "\n",
        "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
        "\n",
        "Les compagnies incluses dans cette base de données sont Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, et American Airlines.\n",
        "\n",
        "Dans le fichier zip du TP, vous trouverez le fichier *airline_tweets_database.csv*, qui est la base de données de tweets que nous allons manipuler.\n",
        "\n",
        "Chaque ligne de ce fichier contient un tweet, avec plusieurs informations : l'identifiant du tweet, l'utilisateur, le contenu, le nombre de retweet... Ainsi que le label.\n",
        "\n",
        "3 labels différents sont possibles dans ce dataset : *négatif*, *neutre* et *positif*, que l'on va représenter respectivement par 0, 1 et 2.\n",
        "\n",
        "Pour ce TP, on ne va conserver ici que le texte et le label. On divise ensuite la base de données en 3 ensembles (entrainement/validation/test). Vous utiliserez l'ensemble d'entraînement et de validation pour cette partie, et l'ensemble de test à la partie suivante.\n",
        "\n",
        "Le code ci-dessous permet de charger ces ensembles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVswRtmYg4pk",
        "colab_type": "code",
        "outputId": "419ec7ab-b214-4bb8-cd03-cd89dc3e09ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_dataset(path):\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
        "        \n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        \n",
        "        # Taking the header of the file + the index of useful columns:\n",
        "        header = next(reader)\n",
        "        ind_label = header.index('airline_sentiment')\n",
        "        ind_text = header.index('text')\n",
        "        \n",
        "        for row in reader:\n",
        "            x.append(row[ind_text])\n",
        "            \n",
        "            label = row[ind_label]\n",
        "            \n",
        "            if label == \"negative\":\n",
        "                y.append(0)\n",
        "            elif label == \"neutral\":\n",
        "                y.append(1)\n",
        "            elif label == \"positive\":\n",
        "                y.append(2)\n",
        "\n",
        "        assert len(x) == len(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# Path of the dataset\n",
        "# path = \"TP1/data/airline_tweets_database.csv\"\n",
        "path = \"drive/My Drive/airline_tweets_database.csv\"\n",
        "\n",
        "X, y = load_dataset(path)\n",
        "\n",
        "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
        "\n",
        "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
        "\n",
        "print(\"Length of training set : \", len(train_X))\n",
        "print(\"Length of validation set : \", len(valid_X))\n",
        "print(\"Length of test set : \", len(test_X))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training set :  10204\n",
            "Length of validation set :  2240\n",
            "Length of test set :  2196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaKtnUxg4pn",
        "colab_type": "text"
      },
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "Nous allons ici implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP. Ensuite, afin d'avoir un modèle qui s'adapte mieux au format de Twitter, nous ajouterons une étape spécifique supplémentaire.\n",
        "\n",
        "### 3.1. Tokenization\n",
        "\n",
        "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
        "\n",
        "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
        "\n",
        "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
        "\n",
        "\n",
        "##### Question 1. Implémentez les 2 tokenizers différents suivants: (0.5 Pts)\n",
        "\n",
        "- Le **SpaceTokenizer**, qui est un tokenizer naïf qui sépare simplement en fonction des espaces.\n",
        "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tRU0f6Sg4po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "class SpaceTokenizer(object):\n",
        "    \"\"\"\n",
        "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
        "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
        "\n",
        "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
        "    \"\"\"\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # TODO\n",
        "        tokens = []\n",
        "        tokens = text.split()\n",
        "        # Have to return a list of tokens\n",
        "        return tokens\n",
        "\n",
        "\n",
        "class NLTKTokenizer(object):\n",
        "    \"\"\"\n",
        "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
        "    \"\"\"\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # TODO\n",
        "        tokens = TweetTokenizer().tokenize(text)\n",
        "        # Have to return a list of tokens\n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUJjqEhIg4pr",
        "colab_type": "text"
      },
      "source": [
        "#### Testez les deux tokenizers. Quelles différences pouvez-vous constater?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznpH6DHg4pr",
        "colab_type": "code",
        "outputId": "4d4809c8-8bf7-4cb6-c345-137cb9c2d839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "chaine = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
        "nltkTokenizer = NLTKTokenizer()\n",
        "spaceTokenizer = SpaceTokenizer()\n",
        "print(nltkTokenizer.tokenize(chaine))\n",
        "print(spaceTokenizer.tokenize(chaine))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
            "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgofDks1g4pu",
        "colab_type": "text"
      },
      "source": [
        "Nous pouvons constater plus haut que le tweetTokenizer fait une distinction avec les ponctuations et autres symboles alors que le spaceTokenizer n'en fait pas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwbd_8YPg4pv",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Troncature (ou Stemming)\n",
        "\n",
        "Dans les phrases \"I should have bought a new shoes today\" et \"I spent too much money buying games\", les mots \"buying\" et \"bought\" représentent la même idée. Considérer ces deux mots comme différents ne ferait qu'augmenter pour rien la complexité et la dimension du problème, ce qui peut avoir un impact négatif sur la performance globale. Ainsi, on peut donc plutôt une forme unique (comme la racine du mot) pour représenter ces deux mots de la même manière. Ce procédé de conversion de mots en racines permettant de réduire la dimension est appelé usuellement *stemming*, que l'on peut traduire par troncature.\n",
        "\n",
        "\n",
        "#### Question 2. Récupérez les troncatures des tokens en utilisant l'attribut *stemmer* de la classe *Stemmer* (0.5 Pts) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXucYL5Mg4pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "class Stemmer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "    def stem(self, token):\n",
        "        \"\"\"\n",
        "        tokens: a list of strings\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        stem = self.stemmer.stem(token)\n",
        "        \n",
        "        # Have to return a list of stems\n",
        "        return stem\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCeBGdQlg4pz",
        "colab_type": "code",
        "outputId": "0991c146-f5a0-44b9-d562-654301963afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#for testing by Marc\n",
        "input_str = \"algorithms\"\n",
        "stemmer = Stemmer()\n",
        "stem_output = stemmer.stem(input_str)\n",
        "print (stem_output)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "algorithm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWHM0jImg4p1",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Twitter preprocessing\n",
        "\n",
        "Parfois, appliquer uniquement ces deux étapes ne suffit pas, due aux particularités des données que nous manipulons, qui peuvent demander des étapes de preprocessing spécifiques afin d'obtenir un modèle plus adapté.\n",
        "\n",
        "Couramment en NLP, un dictionnaire est utilisé pour stocker un ensemble de mots, et tous les mots n'appartenant pas au dictionnaire sont considérés comme inconnus. Ainsi, avec ce choix d'implémentation, la dimension de l'espace caractéristique du modèle est directement liée au nombre de mots du dictionnaire. Ainsi, pour des raisons de complexité mais aussi car les modèles à trop grande dimension peuvent souffrir du fléau de la dimensionnalité, il est préférable de réduire la taille de notre vocabulaire.\n",
        "\n",
        "#### Question 3. Donnez, en expliquant brièvement, au moins deux exemples d'étapes de préprocessing qui permettent de réduire la taille du dictionnaire ici, puis implémentez-les.  (2.0 points)\n",
        "\n",
        "Ces étapes de préprocessing doivent être en rapport aux charactéristiques spécifiques des données de Twitter. La suppression des mots vides ne compte pas comme une des deux étapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6efYTMMg4p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "class TwitterPreprocessing(object):\n",
        "\n",
        "    def preprocess(self, tweet):\n",
        "        \"\"\"\n",
        "        tweet: original tweet\n",
        "        \"\"\"\n",
        "        # TODO : Write your preprocessing steps here.\n",
        "\n",
        "        #remove punctuations, numbers , special characters\n",
        "        #remove URls            \n",
        "        for w in tweet: \n",
        "            if w in string.punctuation: \n",
        "                tweet.remove(w)\n",
        "            if(re.match(r\"http*\", w)):\n",
        "                tweet.remove(w)\n",
        "        new_tweet = tweet\n",
        "        # return the preprocessed twitter\n",
        "        return new_tweet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcYk0jj7g4p5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu4zEigmg4p7",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.  Pipeline\n",
        "\n",
        "Une pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer, les troncatures et le preprocessing spécifique à Twitter. \n",
        "\n",
        "**N'hésitez pas à changer l'ordre des étapes de preprocessing si vous le souhaitez.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydcmz0xJg4p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreprocessingPipeline:\n",
        "\n",
        "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
        "        \"\"\"\n",
        "        tokenization: enable or disable tokenization.\n",
        "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
        "        stemming: enable or disable stemming.\n",
        "        \"\"\"\n",
        "\n",
        "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
        "        self.twitterPreprocesser = TwitterPreprocessing(\n",
        "        ) if twitterPreprocessing else None\n",
        "        self.stemmer = Stemmer() if stemming else None\n",
        "\n",
        "    def preprocess(self, tweet):\n",
        "        \"\"\"\n",
        "        Transform the raw data\n",
        "\n",
        "        tokenization: boolean value.\n",
        "        twitterPreprocessing: boolean value. Apply the\n",
        "        stemming: boolean value.\n",
        "        \"\"\"\n",
        "        tokens = self.tokenizer.tokenize(tweet)\n",
        "        if self.stemmer:\n",
        "            tokens = list(map(self.stemmer.stem, tokens))\n",
        "        if self.twitterPreprocesser:\n",
        "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Rli_1rg4p_",
        "colab_type": "text"
      },
      "source": [
        "Test de la pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVEQxwcHg4p_",
        "colab_type": "code",
        "outputId": "8a4870d5-8a22-40e7-cf10-e2ddea6ed7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
        "print(list(map(pipeline.preprocess, train_X[:18])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg'], ['@southwestair', 'hi', 'i', 'just', 'saw', 'a', 'black', 'histori', 'month', 'commerci', 'on', 'tv', 'im', 'excit', 'in', 'support', 'of', 'this', 'month', 'will', 'you', 'all', 'grant', 'me', '1', 'free', 'trip'], ['@southwestair', 'hope', 'you', 'answer', 'the', 'phone', 'today'], ['@jetblu', 'not', 'make', 'a', 'great', 'first', 'impress', 'on', 'my', 'first', 'flight', '20', 'minut', 'before', 'board', 'and', 'the', 'gate', 'agent', 'still', \"can't\", 'assign', 'me', 'a', 'seat'], ['@americanair', \"wasn't\", 'offer', 'a', 'flight', 'out', 'of', 'phl', 'until', 'tuesday', 'so', 'had', 'to', 'ask', 'to', 'be', 'book', 'to', 'houston', 'instead', 'of', 'austin'], ['@unit', 'what', 'the', 'hell', 'flight', '746', 'delay', 'sinc', '3', 'pm', 'final', 'board', 'and', 'now', 'sit', 'on', 'the', 'tarmac', 'is', 'this', 'f', '&', '$', 'cking', 'plane', 'ever', 'leav'], ['@usairway', 'after', 'miss', 'my', 'flight', 'and', 'reflight', 'book', 'problem', '2x', 'i', 'just', 'walk', 'onto', 'anoth', 'flight', 'and', 'my', 'phone', 'was', 'still', 'on', 'the', 'seat', '!'], ['@americanair', 'thx', 'i', 'hope', 'so', 'iah', 'to', 'dfw', 'to', 'okc', 'has', 'turn', 'out', 'to', 'be', 'a', 'long', 'trip', 'today', 'and', 'i', 'have', 'to', 'work', 'tomorrow'], ['@southwestair', 'broke', 'the', 'stroller', 'my', 'wife', 'and', 'babi', 'gate', 'check', 'they', 'told', 'her', \"it's\", 'not', 'their', 'problem', 'call', 'the', 'a', 'list', 'prefer', 'line', 'now'], ['@americanair', 'how', 'is', 'us4623', 'go', 'to', 'be', 'on', 'time', 'when', 'they', 'are', 'still', 'deplan', 'at', '4:08', 'this', 'is', 'bs'], ['@usairway', 'my', 'flight', 'book', 'problem', 'c68ld9', 'just', 'time', 'out', 'when', 'i', 'select', 'it', 'under', 'manag', 'my', 'flight', 'book', 'problem', 'for', 'month', 'now', 'i', 'have', 'email', 'but', 'no', 'respons', 'help'], ['@jetblu', 'you', 'guy', 'realli', 'suck', 'i', 'just', 'spent', '40', 'minut', 'on', 'the', 'phone', 'link', 'my', 'credit', 'to', 'my', 'account', 'they', 'are', 'still', 'not', 'there', '1/2'], ['@americanair', 'check', 'on', 'what', 'our', 'broken', 'tablet', 'see', 'attach', 'pictur', '#media', '#filmcrew', '#nbc', '#cnn'], ['@unit', \"i'm\", 'confus', 'after', 'your', '@dulles_airport', 'agent', 'direct', 'us', 'to', 'go', 'to', 'your', 'websit', 'after', 'delay', 'noth', 'work', 'http://t.co/cyprhe5gok'], ['@virginamerica', 'has', 'the', 'most', 'incred', 'custom', 'servic', \"i'v\", 'ever', 'experienc', 'so', 'refresh'], ['@americanair', 'you', 'should', 'be', 'apolog', 'for', 'your', 'rude', 'sale', 'rep', 'and', 'failur', 'to', 'offer', 'anyth', 'other', 'than', 'trite', 'condescend', 'platitud', '...'], ['@americanair', 'aww', 'thank', 'aa', '..', 'dfw', 'was', 'on', 'gma', 'up', 'here', 'this', 'am', '..', 'so', 'i', 'understand', '..', 'btw', 'a', 'a', 'is', 'my', 'airlin', 'when', 'im', 'abl', 'to', 'trv', '..', 'love', 'you', 'guy', ':)'], ['@americanair', '740pm', 'wheel', 'up', 'to', 'be', 'exact', \"i'm\", 'send', 'a', 'dm']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcVUMT4jg4qC",
        "colab_type": "text"
      },
      "source": [
        "## 4. N-grams\n",
        "\n",
        "Un n-gram est une séquence continue de *n* tokens dans un texte. Par exemple, les séquences *\"nous a\"* et *\"la porte\"* sont deux exemples de 2-grams de la phrase *\"Il nous a dit au revoir en franchissant la porte.\"*. 1-gram, 2-gram et 3-gram sont respectivement appelés unigram, bigram et trigram. \n",
        "\n",
        "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
        "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
        "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
        "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n",
        "\n",
        "\n",
        "##### Question 4. Implementez les fonctions `bigram` et `trigram`. (1 Pt)\n",
        "\n",
        "Vous devez résoudre cette question sans utiliser de libraire exterieur comme scikit-learn par exemple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEysiHs-g4qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram(tokens):\n",
        "    \"\"\"\n",
        "    tokens: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    ngrams = zip(*[tokens[i:] for i in range(2)])\n",
        "    bigrams = [\" \".join(ngram) for ngram in ngrams]\n",
        "    # This function returns the list of bigrams\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def trigram(tokens):\n",
        "    \"\"\"\n",
        "    tokens: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    ngrams = zip(*[tokens[i:] for i in range(3)])\n",
        "    trigrams = [\" \".join(ngram) for ngram in ngrams]\n",
        "    # This function returns the list of trigrams\n",
        "    return trigrams\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3aD3Mnog4qG",
        "colab_type": "text"
      },
      "source": [
        "test bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i2WnHqrg4qG",
        "colab_type": "code",
        "outputId": "0136fc1a-e8a3-47aa-9c13-bf3af2771a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tokens = ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
        "print(bigram(tokens))\n",
        "print(trigram(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
            "['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcA_I2q1g4qJ",
        "colab_type": "text"
      },
      "source": [
        "## 5. Bag-of-words\n",
        "\n",
        "Régressions logistiques, SVM et d'autres modèles très courants demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
        "\n",
        "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Pandemic is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIGXajgtg4qK",
        "colab_type": "text"
      },
      "source": [
        "|<i></i>     | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-GZTDlPg4qL",
        "colab_type": "text"
      },
      "source": [
        "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
        "\n",
        "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
        "\n",
        "\n",
        "##### Question 5. Implémentez le Bag-of-Words  (2 Pts)\n",
        "\n",
        "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSdnNI4ig4qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "\n",
        "class CountBoW(object):\n",
        "\n",
        "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
        "        \"\"\"\n",
        "        pipelineObj: instance of PreprocesingPipeline\n",
        "        bigram: enable or disable bigram\n",
        "        trigram: enable or disable trigram\n",
        "        words: list of words in the vocabulary\n",
        "        \"\"\"\n",
        "        self.pipeline = pipeline\n",
        "        self.bigram = bigram\n",
        "        self.trigram = trigram\n",
        "        self.words = None\n",
        "    #A changer \n",
        "    def computeBoW(self, X):\n",
        "        print('je commence')\n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        cs_matrix = None\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for tweet in X:\n",
        "            for word in self.words:\n",
        "                count = 0\n",
        "                if word in tweet:\n",
        "                    count = count + 1\n",
        "                    rows.append(X.index(tweet))\n",
        "                    cols.append(self.words.index(word))\n",
        "                    data.append(count)\n",
        "        cs_matrix = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.words)))\n",
        "                \n",
        "        \"\"\"\n",
        "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
        "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
        "        \n",
        "        Entrée: X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        print('je fini')\n",
        "        return cs_matrix\n",
        "        # TODO\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
        "        \n",
        "        Entrée: X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        X = list(map(pipeline.preprocess, X))\n",
        "        #construire le dictionnaire \n",
        "        self.words = []\n",
        "        for sentence in X : \n",
        "            for word in sentence :\n",
        "                if word not in self.words :\n",
        "                    self.words.append(word)\n",
        "            if self.bigram:\n",
        "                bigrams = bigram(sentence)\n",
        "                for element in bigrams:\n",
        "                    if element not in self.words : \n",
        "                        self.words.append(element)\n",
        "            if self.trigram:\n",
        "                trigrams = trigram(sentence)\n",
        "                for element in trigrams:\n",
        "                    if element not in self.words :\n",
        "                        self.words.append(element)\n",
        "        return self.computeBoW(X)\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
        "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
        "\n",
        "        Entrée: X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        X = list(map(pipeline.preprocess, X))\n",
        "        \n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        return self.computeBoW(X)\n",
        "        # TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42ZqnOag4qR",
        "colab_type": "code",
        "outputId": "7ea692a8-80f9-4fc8-fa5d-64ebebda3353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
        "tfidbow = CountBoW(pipeline)\n",
        "print(tfidbow.fit_transform(train_X[:5]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "je commence\n",
            "je fini\n",
            "  (0, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 12)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 15)\t1\n",
            "  (1, 16)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 18)\t1\n",
            "  (1, 19)\t1\n",
            "  (1, 20)\t1\n",
            "  (1, 21)\t1\n",
            "  (1, 22)\t1\n",
            "  (1, 23)\t1\n",
            "  :\t:\n",
            "  (3, 54)\t1\n",
            "  (3, 55)\t1\n",
            "  (3, 56)\t1\n",
            "  (3, 57)\t1\n",
            "  (3, 58)\t1\n",
            "  (3, 59)\t1\n",
            "  (4, 3)\t1\n",
            "  (4, 5)\t1\n",
            "  (4, 9)\t1\n",
            "  (4, 18)\t1\n",
            "  (4, 29)\t1\n",
            "  (4, 60)\t1\n",
            "  (4, 61)\t1\n",
            "  (4, 62)\t1\n",
            "  (4, 63)\t1\n",
            "  (4, 64)\t1\n",
            "  (4, 65)\t1\n",
            "  (4, 66)\t1\n",
            "  (4, 67)\t1\n",
            "  (4, 68)\t1\n",
            "  (4, 69)\t1\n",
            "  (4, 70)\t1\n",
            "  (4, 71)\t1\n",
            "  (4, 72)\t1\n",
            "  (4, 73)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btvPwrpWg4qU",
        "colab_type": "text"
      },
      "source": [
        "## 6. TF-IDF\n",
        "\n",
        "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de tweets de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des tweets sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
        "\n",
        "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
        "\n",
        "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
        "\n",
        "L'IDF d'un mot se calcule de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
        "\\end{equation}\n",
        "\n",
        "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
        "\n",
        "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
        "\\end{equation}\n",
        "\n",
        "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Question 6. Implémentez le bag-of-words avec la pondération de TF-IDF (3 Pts)\n",
        "\n",
        "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzvuLjnMg4qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "\n",
        "\n",
        "class TFIDFBoW(object):\n",
        "\n",
        "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
        "        \"\"\"\n",
        "        pipelineObj: instance of PreprocesingPipeline\n",
        "        bigram: enable or disable bigram\n",
        "        trigram: enable or disable trigram\n",
        "        words: list of words in the vocabulary\n",
        "        idf: list of idfs for each document\n",
        "        \"\"\"\n",
        "        self.pipeline = pipeline\n",
        "        self.bigram = bigram\n",
        "        self.trigram = trigram\n",
        "        self.words = None\n",
        "        self.idf = None\n",
        "    def get_occurence_in_document(self,X,word):\n",
        "        count = 0\n",
        "        for sentence in X:\n",
        "            if word in sentence:\n",
        "                count = count +1\n",
        "        return count        \n",
        "     \n",
        "    def get_frequence_word_in_sentence(self, sentence, word):\n",
        "        return sentence.count(word)/len(sentence)\n",
        "        \n",
        "    \n",
        "    def computeTFIDF(self, X): \n",
        "        print('je commmence')\n",
        "        cs_matrix = None\n",
        "        \"\"\"\n",
        "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
        "        liste de tweets.\n",
        "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
        "        calculé le vecteur contenant l'idf pour chaque document.\n",
        "        \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for sentence in X :\n",
        "            sentence_idfs = self.idf[X.index(sentence)]\n",
        "            for word in self.words:\n",
        "                if word in sentence:\n",
        "                    tdidf = self.get_frequence_word_in_sentence(sentence,word) * sentence_idfs[sentence.index(word)]\n",
        "                    rows.append(X.index(sentence))\n",
        "                    cols.append(self.words.index(word))\n",
        "                    data.append(tdidf)\n",
        "                    \n",
        "        cs_matrix = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.words)))\n",
        "        return cs_matrix\n",
        "\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        print('je suis dans fit')\n",
        "        X = list(map(pipeline.preprocess, X))\n",
        "        #construire le dictionnaire \n",
        "        self.words = []\n",
        "        for sentence in X : \n",
        "            for word in sentence :\n",
        "                if word not in self.words:\n",
        "                    self.words.append(word)\n",
        "            if self.bigram:\n",
        "                bigrams = bigram(sentence)\n",
        "                for element in bigrams:\n",
        "                    if element not in self.words:\n",
        "                        self.words.append(element)\n",
        "            if self.trigram:\n",
        "                trigrams = trigram(sentence)\n",
        "                for element in trigrams:\n",
        "                    if element not in self.words:\n",
        "                        self.words.append(element)\n",
        "#         print(\"mots\\n\")\n",
        "#         print(self.words)\n",
        "        #construire le vecteur des idfs\n",
        "        self.idf = []\n",
        "        for sentence in X :\n",
        "            sentence_idfs = []\n",
        "            for word in sentence : \n",
        "                if word in self.words:\n",
        "                    sentence_idfs.append(math.log(len(X)/self.get_occurence_in_document(X,word)))\n",
        "                else:\n",
        "                    sentence_idfs.append(0)\n",
        "            self.idf.append(sentence_idfs)\n",
        "#                     self.idf.append(sentence_idfs)\n",
        "\n",
        "        tf_idfs = self.computeTFIDF(X)\n",
        "        return tf_idfs\n",
        "                \n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
        "        \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def transform(self, X):\n",
        "        self.idf = []\n",
        "        for sentence in X :\n",
        "            sentence_idfs = []\n",
        "            for word in sentence : \n",
        "                if word in self.words:\n",
        "                    sentence_idfs.append(math.log(len(X)/self.get_occurence_in_document(X,word)))\n",
        "                else:\n",
        "                    sentence_idfs.append(0)\n",
        "            self.idf.append(sentence_idfs)\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
        "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
        "            \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "\n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        tf_idfs = self.computeTFIDF(X)\n",
        "        return tf_idfs\n",
        "        # TODO\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pM-ZIw_Og4qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
        "tfidbow = TFIDFBoW(pipeline)\n",
        "# print(train_X[:5])\n",
        "print(tfidbow.fit_transform(train_X[:5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABWRnizRg4qf",
        "colab_type": "text"
      },
      "source": [
        "## 7. Classification utilisant BoW\n",
        "\n",
        "Pour la classification, nous allons effectuer une régression logisitique (vu en cours ou que vous allez voir bientôt). \n",
        "Pour en savoir plus : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "La méthode `train_evaluate` entraîne et évalue le modèle de régression logistique.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAlUVUVDg4qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
        "    \"\"\"\n",
        "    training_X: tweets from the training dataset\n",
        "    training_Y: tweet labels from the training dataset\n",
        "    validation_X: tweets from the validation dataset\n",
        "    validation_Y: tweet labels from the validation dataset\n",
        "    bowObj: Bag-of-word object\n",
        "    \n",
        "    :return: the classifier and its accuracy in the training and validation dataset.\n",
        "    \"\"\"\n",
        "    print('je commence train')\n",
        "    classifier = LogisticRegression(solver='liblinear',n_jobs=-1)\n",
        "#     print('classifier', classifier)\n",
        "\n",
        "    training_rep = bowObj.fit_transform(training_X)\n",
        "    \n",
        "    print(\"bow object instancied\")\n",
        "    \n",
        "    classifier.fit(training_rep, training_Y)\n",
        "\n",
        "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
        "    validationAcc = accuracy_score(\n",
        "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
        "    print('je finis train')\n",
        "\n",
        "    return classifier, trainAcc, validationAcc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQSGHWVZg4qj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Question 7. Entraînez et calculez la précision de la régression logistique sur les ensembles d'entraînement et de validation. (4 points)\n",
        "\n",
        "Essayez les configurations suivantes :\n",
        "\n",
        "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
        "    2. CountBoW + NLTKTokenizer + unigram\n",
        "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
        "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
        "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
        "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
        "\n",
        "Outre la précision, reportez la taille du dictionnaire pour chacune des configurations. Enfin, décrivez vos résultats obtenus et répondez aux questions suivantes:\n",
        "- Quelles étapes de preprocessing ont effectivement aidé le modèle ? Pourquoi ?\n",
        "- La pondération avec TF-IDF a-t-elle aidé à obtenir une meilleure performance que le simple BoW ?\n",
        "- Les bigrams et trigrams ont-ils amélioré la performance ? Expliquez pourquoi.\n",
        "\n",
        "Indiquez quelle est la configuration que vous choisissez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCD69snJg4qk",
        "colab_type": "code",
        "outputId": "7363f6b9-65bd-42f8-e88f-e70ebb176f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "#elements for configuration first:countbow, second: tokenization, third:twitter preprocessing, fourth:stemming,\n",
        "#fifth:bigram, six:trigram \n",
        "configurations = [[False,False,False,False,False,False],[False,True,False,False,False,False],[True,True,False,True,False,False],\\\n",
        "                 [True,True,True,True,False,False],[True,True,True,True,True,False],[True,True,True,True,True,True]]\n",
        "for configuration in configurations:\n",
        "        print(\"start training for model \" + str(configurations.index(configuration)))\n",
        "        pipeline = PreprocessingPipeline(configuration[1], configuration[2], configuration[3])\n",
        "        if configuration[0] == True:\n",
        "            bowObj = TFIDFBoW(pipeline, configuration[4], configuration[5])\n",
        "        else:\n",
        "            bowObj = CountBoW(pipeline, configuration[4], configuration[5])\n",
        "\n",
        "        classifier,train_acc, validate_acc = train_evaluate(train_X,train_Y,valid_X,valid_Y,bowObj)\n",
        "        print (\"for  model \" + str(configurations.index(configuration)) + \" the train accuracy is: \" + str(train_acc) + \" and the validation accuracy is :\" + str(validate_acc))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training for model 0\n",
            "je commence train\n",
            "je commence\n",
            "je fini\n",
            "bow object instancied\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "je commence\n",
            "je fini\n",
            "je finis train\n",
            "for  model 0 the train accuracy is: 0.9568796550372403 and the validation accuracy is :0.7915178571428572\n",
            "start training for model 1\n",
            "je commence train\n",
            "je commence\n",
            "je fini\n",
            "bow object instancied\n",
            "je commence\n",
            "je fini\n",
            "je finis train\n",
            "for  model 1 the train accuracy is: 0.9399255194041553 and the validation accuracy is :0.8053571428571429\n",
            "start training for model 2\n",
            "je commence train\n",
            "je suis dans fit\n",
            "je commmence\n",
            "bow object instancied\n",
            "je commmence\n",
            "je finis train\n",
            "for  model 2 the train accuracy is: 0.8529988239905919 and the validation accuracy is :0.6209821428571428\n",
            "start training for model 3\n",
            "je commence train\n",
            "je suis dans fit\n",
            "je commmence\n",
            "bow object instancied\n",
            "je commmence\n",
            "je finis train\n",
            "for  model 3 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n",
            "start training for model 4\n",
            "je commence train\n",
            "je suis dans fit\n",
            "je commmence\n",
            "bow object instancied\n",
            "je commmence\n",
            "je finis train\n",
            "for  model 4 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n",
            "start training for model 5\n",
            "je commence train\n",
            "je suis dans fit\n",
            "je commmence\n",
            "bow object instancied\n",
            "je commmence\n",
            "je finis train\n",
            "for  model 5 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t77sO-8bg4qn",
        "colab_type": "text"
      },
      "source": [
        "###### II/ Prototype (7 points)\n",
        "\n",
        "Maintenant que nous avons un modèle de classification entraîné pour l'analyse de sentiments, nous pouvons l'appliquer à notre ensemble de tests et analyser le résultat.\n",
        "\n",
        "## 1. Analyse de Sentiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFHFdRjcg4qo",
        "colab_type": "text"
      },
      "source": [
        "##### Question 9. Implémentez la fonction `detect_airline` qui détecte la compagnie aérienne d'un tweet. (1,5 points)\n",
        "\n",
        "Expliquez votre approche, et les inconvénients possibles.\n",
        "\n",
        "**Attention :** `detect_airline` doit être en mesure de gérer le cas où aucune compagnie n'est mentionnée (auquel cas `None` est retounée), mais aussi le cas où plusieurs compagnies sont mentionnées dans un tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7UUdYFAg4qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_airline(tweet):\n",
        "    companies_term = [\"Virgin America\",\"United Airline\",\"Southwest Airlines\", \"jetBlue\", \"USAirways\", \"American Airlines\"]\n",
        "    mentionned_companies = []\n",
        "    \"\"\"\n",
        "    Detect and return the airline companies mentioned in the tweet\n",
        "    \n",
        "    tweet: represents the tweet message. You should define the data type\n",
        "    \n",
        "    Return: list of detected airline companies\n",
        "    \"\"\"\n",
        "    for term in companies_term:\n",
        "        if term in tweet:\n",
        "            mentionned_companies.append(term)\n",
        "    return mentionned_companies\n",
        "    # TODO\n",
        "t ='@USAirways absolutely worst experience of my life at PHL. 6 hrs, @jetBlue 1 failed departure, 3 gates, and ZERO communication or help!!'\n",
        "d = detect_airline(t)\n",
        "print(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IopdH60Og4qs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Question 10. Implémentez la fonction `extract_sentiment` qui, à partir de tweets et d'un classificateur, extrait leurs sentiments. (0.5 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ0_wKJwg4qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_sentiment(classifier, tweets):\n",
        "    \"\"\"\n",
        "    Extract the tweet sentiment\n",
        "    \n",
        "    classifier: classifier object\n",
        "    tweet: represents the tweet message. You should define the data type\n",
        "    \n",
        "    Return: list of detected airline companies\n",
        "    \"\"\"\n",
        "    # TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMEG4m6lg4qw",
        "colab_type": "text"
      },
      "source": [
        "##### Question 11. En utilisant `extract_tweet_content`, `detect_airline` et `extract_sentiment`, générez un diagramme en bar contenant le nombre de tweets positives, négatifs et neutres pour chacune des compagnies. (2 points)\n",
        "\n",
        "Décrivez brièvement le diagramme et analysez les résultats (par exemple, quelle est la compagnie avec le plus de tweets négatifs?). Expliquez comment un tel diagramme peut aider des compagnies aériennes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE77N5R3g4qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMdg84mKg4qz",
        "colab_type": "text"
      },
      "source": [
        "## 2. Analyse de termes\n",
        "\n",
        "Le POS-tagging (pour *part-of-speech tagging*, en français étiquetage grammatical) consiste à l'extraction de l'information grammaticale d'un token dans une phrase. Par exemple, la table ci-dessous donne un exemple du *POS-tagging* de la phrase *\"The cat is white!\"*\n",
        "\n",
        "\n",
        "|   The   | cat  |  is  | white     |    !       |\n",
        "|---------|------|------|-----------|------------|\n",
        "| article | noun | verb | adjective | punctation |\n",
        "\n",
        "\n",
        "Pour autant, le *POS-tagging* peut être plus complexe que les règles simples apprises à l'école. Il faut souvent des informations plus détaillées sur le rôle d'un terme dans une phrase. Pour notre problème, nous n'avons pas besoin d'utiliser un modèle linguistique plus complexe, nous allons utiliser ce qu'on appelle des *POS-tags* universelles.\n",
        "\n",
        "En *POS-tagging*, chaque token est représenté par un tag. La liste des POS-tags utilisés sont disponibles ici :\n",
        "https://universaldependencies.org/u/pos/ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Spx4Mag4q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK POS-tagger\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "#before using pos_tag function, you have to tokenize the sentence.\n",
        "s = ['The', 'cat', 'is',  'white', '!']\n",
        "nltk.pos_tag(s,tagset='universal')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBXeIH7Ug4q2",
        "colab_type": "text"
      },
      "source": [
        "##### Question 12. Implémentez un code qui collecte les 10 termes les plus fréquents pour chaque compagnie aérienne. (2 Pts)\n",
        "\n",
        "Ici, vous n'allez considérer que les termes apparaissant dans les tweets positifs et négatifs. \n",
        "\n",
        "De plus, nous allons utiliser la définition suivante de \"terme\":\n",
        "\n",
        "1. Un mot qui est soit un adjectif, soit un nom\n",
        "2. Un N-gram composé d'adjectifs suivit par un nom (par exemple, \"nice place\"), ou un nom suivi par un autre nom (par exemple, \"sports club\").\n",
        "\n",
        "Ensuite, **générez une table** contenant les 10 termes les plus fréquents, avec leurs fréquences (en pourcentage) pour chaque compagnie.\n",
        "\n",
        "*N'oubliez pas de supprimer le nom de la compagnie parmi les termes !*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zuuk_9Qg4q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCuICldzg4q7",
        "colab_type": "text"
      },
      "source": [
        "##### Question 13. Que conclure de la table généré à la question 12 pour les compagnies ? (1 Pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbiNzWueg4q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sFCAAccg4q-",
        "colab_type": "text"
      },
      "source": [
        "# III/ Bonus (2 points)\n",
        "\n",
        "Les noms de personnes, les noms de sociétés et les emplacements sont appelés \"entités nommées\". La reconnaissance d'entité nommée (NER, pour *Named-entity recognition*) consiste à extraire les entités nommées en les classant à l'aide de catégories prédéfinies. Dans cette section bonus, vous utiliserez un outil de NER pour extraire automatiquement des entités nommées des tweets. Cette approche est suffisamment générique pour récupérer des informations sur d’autres sociétés ou même des noms de produits et de personnes.\n",
        "\n",
        "\n",
        "**Pour le bonus, vous êtes libres d'utiliser n'importe quel NER implémenté en Python.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwmEecqQg4q-",
        "colab_type": "text"
      },
      "source": [
        "##### Question Bonus 1.  Implémentez un code qui génère une table contenant le top 10 des NER de la base de données. (1 point)\n",
        "\n",
        "Cette table doit contenir les fréquences des entités nommées. Ensuite, générez un diagramme en bar qui montre le nombre de tweets positifs, négatifs ou neutres pour chacunes des 10 NER. Décrivez le résultat obtenu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8V5jAWjg4q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToIUiL6Ug4rB",
        "colab_type": "text"
      },
      "source": [
        "##### Question Bonus 2. Générez une table similaire à la question 12 pour le top 10 des NER pour chaque compagnie. (1 point)\n",
        "\n",
        "Que peut-on conclure de ces résultats?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyf05Kcvg4rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}