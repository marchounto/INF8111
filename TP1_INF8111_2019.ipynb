{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FrOJucfg4pZ"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 Automne 2019 - Preprocessing de tweets pour de l'analyse de sentiments\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOrkwKxwg4pb"
   },
   "source": [
    "## Présentation\n",
    "\n",
    "Twitter est un réseau social permettant aux utilisateurs de publier des informations et communiquer entre eux par le biais de messages, appelés tweets, pouvant contenir jusqu'à 280 caractères. Largement utilisé aujourd'hui, ce réseau peut être un outil pour des entreprises qui souhaitent évaluer l'avis de leurs clients.\n",
    "\n",
    "Dans ce TP, on se met à la place d'une compagnie aérienne, qui souhaiterait détecter les tweets qui la mentionnent et analyser si ce sont des mentions positives ou négatives, en comparant leur résultat avec les autres compagnies.\n",
    "\n",
    "Le *preprocessing* est une tâche cruciale en fouille de données. Elle permet de transformer les données brutes en un format adapté à l'application de méthodes de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hide_input": true,
    "id": "r_2OPLwjg4pc"
   },
   "source": [
    "# I/ Analyse de sentiments (13 Pts)\n",
    "\n",
    "Usuellement dans la littérature, la tâche d'extraire le sentiment d'un texte est appelé *sentiment analysis*.\n",
    "Ici pour se faire, nous allons utiliser un modèle *bag-of-words* (BoW).\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
    "\n",
    "Installez les libraires en question et exécutez le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FoZsWXCkg4pd",
    "outputId": "301e81a6-e447-47e6-8b9d-c7edcdfbaa02"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9v1hYCG6g4pg",
    "outputId": "9e67ab86-7790-46fd-ce38-9c3eaefc866f"
   },
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# !pip install --user numpy\n",
    "# !pip install --user sklearn\n",
    "# !pip install --user scipy\n",
    "# !pip install --user nltk\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhC1VWBgg4pj"
   },
   "source": [
    "## 2. Jeu de données\n",
    "\n",
    "On utilise un jeu de donnée provenant de *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Pour citer la source originale de la base :\n",
    "\n",
    "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "Les compagnies incluses dans cette base de données sont Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, et American Airlines.\n",
    "\n",
    "Dans le fichier zip du TP, vous trouverez le fichier *airline_tweets_database.csv*, qui est la base de données de tweets que nous allons manipuler.\n",
    "\n",
    "Chaque ligne de ce fichier contient un tweet, avec plusieurs informations : l'identifiant du tweet, l'utilisateur, le contenu, le nombre de retweet... Ainsi que le label.\n",
    "\n",
    "3 labels différents sont possibles dans ce dataset : *négatif*, *neutre* et *positif*, que l'on va représenter respectivement par 0, 1 et 2.\n",
    "\n",
    "Pour ce TP, on ne va conserver ici que le texte et le label. On divise ensuite la base de données en 3 ensembles (entrainement/validation/test). Vous utiliserez l'ensemble d'entraînement et de validation pour cette partie, et l'ensemble de test à la partie suivante.\n",
    "\n",
    "Le code ci-dessous permet de charger ces ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "MVswRtmYg4pk",
    "outputId": "419ec7ab-b214-4bb8-cd03-cd89dc3e09ec"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('airline_sentiment')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            \n",
    "            if label == \"negative\":\n",
    "                y.append(0)\n",
    "            elif label == \"neutral\":\n",
    "                y.append(1)\n",
    "            elif label == \"positive\":\n",
    "                y.append(2)\n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "# path = \"TP1/data/airline_tweets_database.csv\"\n",
    "path = \"drive/My Drive/airline_tweets_database.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPaKtnUxg4pn"
   },
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Nous allons ici implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP. Ensuite, afin d'avoir un modèle qui s'adapte mieux au format de Twitter, nous ajouterons une étape spécifique supplémentaire.\n",
    "\n",
    "### 3.1. Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
    "\n",
    "\n",
    "##### Question 1. Implémentez les 2 tokenizers différents suivants: (0.5 Pts)\n",
    "\n",
    "- Le **SpaceTokenizer**, qui est un tokenizer naïf qui sépare simplement en fonction des espaces.\n",
    "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tRU0f6Sg4po"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # TODO\n",
    "        tokens = []\n",
    "        tokens = text.split()\n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # TODO\n",
    "        tokens = TweetTokenizer().tokenize(text)\n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUJjqEhIg4pr"
   },
   "source": [
    "#### Testez les deux tokenizers. Quelles différences pouvez-vous constater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sznpH6DHg4pr",
    "outputId": "4d4809c8-8bf7-4cb6-c345-137cb9c2d839"
   },
   "outputs": [],
   "source": [
    "chaine = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "nltkTokenizer = NLTKTokenizer()\n",
    "spaceTokenizer = SpaceTokenizer()\n",
    "print(nltkTokenizer.tokenize(chaine))\n",
    "print(spaceTokenizer.tokenize(chaine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgofDks1g4pu"
   },
   "source": [
    "Nous pouvons constater plus haut que le tweetTokenizer fait une distinction avec les ponctuations et autres symboles alors que le spaceTokenizer n'en fait pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fwbd_8YPg4pv"
   },
   "source": [
    "### 3.2. Troncature (ou Stemming)\n",
    "\n",
    "Dans les phrases \"I should have bought a new shoes today\" et \"I spent too much money buying games\", les mots \"buying\" et \"bought\" représentent la même idée. Considérer ces deux mots comme différents ne ferait qu'augmenter pour rien la complexité et la dimension du problème, ce qui peut avoir un impact négatif sur la performance globale. Ainsi, on peut donc plutôt une forme unique (comme la racine du mot) pour représenter ces deux mots de la même manière. Ce procédé de conversion de mots en racines permettant de réduire la dimension est appelé usuellement *stemming*, que l'on peut traduire par troncature.\n",
    "\n",
    "\n",
    "#### Question 2. Récupérez les troncatures des tokens en utilisant l'attribut *stemmer* de la classe *Stemmer* (0.5 Pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXucYL5Mg4pw"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, token):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        stem = self.stemmer.stem(token)\n",
    "        \n",
    "        # Have to return a list of stems\n",
    "        return stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wCeBGdQlg4pz",
    "outputId": "0991c146-f5a0-44b9-d562-654301963afc"
   },
   "outputs": [],
   "source": [
    "#for testing by Marc\n",
    "input_str = \"algorithms\"\n",
    "stemmer = Stemmer()\n",
    "stem_output = stemmer.stem(input_str)\n",
    "print (stem_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWHM0jImg4p1"
   },
   "source": [
    "### 3.3. Twitter preprocessing\n",
    "\n",
    "Parfois, appliquer uniquement ces deux étapes ne suffit pas, due aux particularités des données que nous manipulons, qui peuvent demander des étapes de preprocessing spécifiques afin d'obtenir un modèle plus adapté.\n",
    "\n",
    "Couramment en NLP, un dictionnaire est utilisé pour stocker un ensemble de mots, et tous les mots n'appartenant pas au dictionnaire sont considérés comme inconnus. Ainsi, avec ce choix d'implémentation, la dimension de l'espace caractéristique du modèle est directement liée au nombre de mots du dictionnaire. Ainsi, pour des raisons de complexité mais aussi car les modèles à trop grande dimension peuvent souffrir du fléau de la dimensionnalité, il est préférable de réduire la taille de notre vocabulaire.\n",
    "\n",
    "#### Question 3. Donnez, en expliquant brièvement, au moins deux exemples d'étapes de préprocessing qui permettent de réduire la taille du dictionnaire ici, puis implémentez-les.  (2.0 points)\n",
    "\n",
    "Ces étapes de préprocessing doivent être en rapport aux charactéristiques spécifiques des données de Twitter. La suppression des mots vides ne compte pas comme une des deux étapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6efYTMMg4p2"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        # TODO : Write your preprocessing steps here.\n",
    "\n",
    "        #remove punctuations, numbers , special characters\n",
    "        #remove URls            \n",
    "        for w in tweet: \n",
    "            if w in string.punctuation: \n",
    "                tweet.remove(w)\n",
    "            if(re.match(r\"http*\", w)):\n",
    "                tweet.remove(w)\n",
    "        new_tweet = tweet\n",
    "        # return the preprocessed twitter\n",
    "        return new_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcYk0jj7g4p5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gu4zEigmg4p7"
   },
   "source": [
    "### 3.4.  Pipeline\n",
    "\n",
    "Une pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer, les troncatures et le preprocessing spécifique à Twitter. \n",
    "\n",
    "**N'hésitez pas à changer l'ordre des étapes de preprocessing si vous le souhaitez.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ydcmz0xJg4p8"
   },
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "        if self.stemmer:\n",
    "            tokens = list(map(self.stemmer.stem, tokens))\n",
    "        if self.twitterPreprocesser:\n",
    "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Rli_1rg4p_"
   },
   "source": [
    "Test de la pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FVEQxwcHg4p_",
    "outputId": "8a4870d5-8a22-40e7-cf10-e2ddea6ed7d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg'], ['@southwestair', 'hi', 'i', 'just', 'saw', 'a', 'black', 'histori', 'month', 'commerci', 'on', 'tv', 'im', 'excit', 'in', 'support', 'of', 'this', 'month', 'will', 'you', 'all', 'grant', 'me', '1', 'free', 'trip'], ['@southwestair', 'hope', 'you', 'answer', 'the', 'phone', 'today'], ['@jetblu', 'not', 'make', 'a', 'great', 'first', 'impress', 'on', 'my', 'first', 'flight', '20', 'minut', 'before', 'board', 'and', 'the', 'gate', 'agent', 'still', \"can't\", 'assign', 'me', 'a', 'seat'], ['@americanair', \"wasn't\", 'offer', 'a', 'flight', 'out', 'of', 'phl', 'until', 'tuesday', 'so', 'had', 'to', 'ask', 'to', 'be', 'book', 'to', 'houston', 'instead', 'of', 'austin'], ['@unit', 'what', 'the', 'hell', 'flight', '746', 'delay', 'sinc', '3', 'pm', 'final', 'board', 'and', 'now', 'sit', 'on', 'the', 'tarmac', 'is', 'this', 'f', '&', '$', 'cking', 'plane', 'ever', 'leav'], ['@usairway', 'after', 'miss', 'my', 'flight', 'and', 'reflight', 'book', 'problem', '2x', 'i', 'just', 'walk', 'onto', 'anoth', 'flight', 'and', 'my', 'phone', 'was', 'still', 'on', 'the', 'seat', '!'], ['@americanair', 'thx', 'i', 'hope', 'so', 'iah', 'to', 'dfw', 'to', 'okc', 'has', 'turn', 'out', 'to', 'be', 'a', 'long', 'trip', 'today', 'and', 'i', 'have', 'to', 'work', 'tomorrow'], ['@southwestair', 'broke', 'the', 'stroller', 'my', 'wife', 'and', 'babi', 'gate', 'check', 'they', 'told', 'her', \"it's\", 'not', 'their', 'problem', 'call', 'the', 'a', 'list', 'prefer', 'line', 'now'], ['@americanair', 'how', 'is', 'us4623', 'go', 'to', 'be', 'on', 'time', 'when', 'they', 'are', 'still', 'deplan', 'at', '4:08', 'this', 'is', 'bs'], ['@usairway', 'my', 'flight', 'book', 'problem', 'c68ld9', 'just', 'time', 'out', 'when', 'i', 'select', 'it', 'under', 'manag', 'my', 'flight', 'book', 'problem', 'for', 'month', 'now', 'i', 'have', 'email', 'but', 'no', 'respons', 'help'], ['@jetblu', 'you', 'guy', 'realli', 'suck', 'i', 'just', 'spent', '40', 'minut', 'on', 'the', 'phone', 'link', 'my', 'credit', 'to', 'my', 'account', 'they', 'are', 'still', 'not', 'there', '1/2'], ['@americanair', 'check', 'on', 'what', 'our', 'broken', 'tablet', 'see', 'attach', 'pictur', '#media', '#filmcrew', '#nbc', '#cnn'], ['@unit', \"i'm\", 'confus', 'after', 'your', '@dulles_airport', 'agent', 'direct', 'us', 'to', 'go', 'to', 'your', 'websit', 'after', 'delay', 'noth', 'work', 'http://t.co/cyprhe5gok'], ['@virginamerica', 'has', 'the', 'most', 'incred', 'custom', 'servic', \"i'v\", 'ever', 'experienc', 'so', 'refresh'], ['@americanair', 'you', 'should', 'be', 'apolog', 'for', 'your', 'rude', 'sale', 'rep', 'and', 'failur', 'to', 'offer', 'anyth', 'other', 'than', 'trite', 'condescend', 'platitud', '...'], ['@americanair', 'aww', 'thank', 'aa', '..', 'dfw', 'was', 'on', 'gma', 'up', 'here', 'this', 'am', '..', 'so', 'i', 'understand', '..', 'btw', 'a', 'a', 'is', 'my', 'airlin', 'when', 'im', 'abl', 'to', 'trv', '..', 'love', 'you', 'guy', ':)'], ['@americanair', '740pm', 'wheel', 'up', 'to', 'be', 'exact', \"i'm\", 'send', 'a', 'dm']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "print(list(map(pipeline.preprocess, train_X[:18])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcVUMT4jg4qC"
   },
   "source": [
    "## 4. N-grams\n",
    "\n",
    "Un n-gram est une séquence continue de *n* tokens dans un texte. Par exemple, les séquences *\"nous a\"* et *\"la porte\"* sont deux exemples de 2-grams de la phrase *\"Il nous a dit au revoir en franchissant la porte.\"*. 1-gram, 2-gram et 3-gram sont respectivement appelés unigram, bigram et trigram. \n",
    "\n",
    "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
    "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
    "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n",
    "\n",
    "\n",
    "##### Question 4. Implementez les fonctions `bigram` et `trigram`. (1 Pt)\n",
    "\n",
    "Vous devez résoudre cette question sans utiliser de libraire exterieur comme scikit-learn par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEysiHs-g4qD"
   },
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ngrams = zip(*[tokens[i:] for i in range(2)])\n",
    "    bigrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ngrams = zip(*[tokens[i:] for i in range(3)])\n",
    "    trigrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "    # This function returns the list of trigrams\n",
    "    return trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O3aD3Mnog4qG"
   },
   "source": [
    "test bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4i2WnHqrg4qG",
    "outputId": "0136fc1a-e8a3-47aa-9c13-bf3af2771a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
      "['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "print(bigram(tokens))\n",
    "print(trigram(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcA_I2q1g4qJ"
   },
   "source": [
    "## 5. Bag-of-words\n",
    "\n",
    "Régressions logistiques, SVM et d'autres modèles très courants demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Pandemic is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIGXajgtg4qK"
   },
   "source": [
    "|<i></i>     | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-GZTDlPg4qL"
   },
   "source": [
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n",
    "\n",
    "##### Question 5. Implémentez le Bag-of-Words  (2 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSdnNI4ig4qM"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "    def computeBoW(self, X):\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        cs_matrix = None\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for tweet in X:\n",
    "            for word in self.words:\n",
    "                count = 0\n",
    "                if word in tweet:\n",
    "                    count = count + 1\n",
    "                    rows.append(X.index(tweet))\n",
    "                    cols.append(self.words.index(word))\n",
    "                    data.append(count)\n",
    "        cs_matrix = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.words)))\n",
    "                \n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "        \n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        print('je fini')\n",
    "        return cs_matrix\n",
    "        # TODO\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        \n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        #construire le dictionnaire \n",
    "        self.words = []\n",
    "        for sentence in X : \n",
    "            for word in sentence :\n",
    "                if word not in self.words :\n",
    "                    self.words.append(word)\n",
    "            if self.bigram:\n",
    "                bigrams = bigram(sentence)\n",
    "                for element in bigrams:\n",
    "                    if element not in self.words : \n",
    "                        self.words.append(element)\n",
    "            if self.trigram:\n",
    "                trigrams = trigram(sentence)\n",
    "                for element in trigrams:\n",
    "                    if element not in self.words :\n",
    "                        self.words.append(element)\n",
    "        return self.computeBoW(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
    "\n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        return self.computeBoW(X)\n",
    "        # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "e42ZqnOag4qR",
    "outputId": "7ea692a8-80f9-4fc8-fa5d-64ebebda3353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je commence\n",
      "je fini\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 23)\t1\n",
      "  :\t:\n",
      "  (3, 54)\t1\n",
      "  (3, 55)\t1\n",
      "  (3, 56)\t1\n",
      "  (3, 57)\t1\n",
      "  (3, 58)\t1\n",
      "  (3, 59)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 18)\t1\n",
      "  (4, 29)\t1\n",
      "  (4, 60)\t1\n",
      "  (4, 61)\t1\n",
      "  (4, 62)\t1\n",
      "  (4, 63)\t1\n",
      "  (4, 64)\t1\n",
      "  (4, 65)\t1\n",
      "  (4, 66)\t1\n",
      "  (4, 67)\t1\n",
      "  (4, 68)\t1\n",
      "  (4, 69)\t1\n",
      "  (4, 70)\t1\n",
      "  (4, 71)\t1\n",
      "  (4, 72)\t1\n",
      "  (4, 73)\t1\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "tfidbow = CountBoW(pipeline)\n",
    "print(tfidbow.fit_transform(train_X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btvPwrpWg4qU"
   },
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de tweets de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des tweets sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
    "\n",
    "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
    "\\end{equation}\n",
    "\n",
    "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Question 6. Implémentez le bag-of-words avec la pondération de TF-IDF (3 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzvuLjnMg4qV"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        idf: list of idfs for each document\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "    def get_occurence_in_document(self,X,word):\n",
    "        count = 0\n",
    "        for sentence in X:\n",
    "            if word in sentence:\n",
    "                count = count +1\n",
    "        return count        \n",
    "     \n",
    "    def get_frequence_word_in_sentence(self, sentence, word):\n",
    "        return sentence.count(word)/len(sentence)\n",
    "        \n",
    "    \n",
    "    def computeTFIDF(self, X): \n",
    "        cs_matrix = None\n",
    "        \"\"\"\n",
    "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
    "        liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
    "        calculé le vecteur contenant l'idf pour chaque document.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for sentence in X :\n",
    "            sentence_idfs = self.idf[X.index(sentence)]\n",
    "            for word in self.words:\n",
    "                if word in sentence:\n",
    "                    tdidf = self.get_frequence_word_in_sentence(sentence,word) * sentence_idfs[sentence.index(word)]\n",
    "                    rows.append(X.index(sentence))\n",
    "                    cols.append(self.words.index(word))\n",
    "                    data.append(tdidf)\n",
    "                    \n",
    "        cs_matrix = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.words)))\n",
    "        return cs_matrix\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        print('je suis dans fit')\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        #construire le dictionnaire \n",
    "        self.words = []\n",
    "        for sentence in X : \n",
    "            for word in sentence :\n",
    "                if word not in self.words:\n",
    "                    self.words.append(word)\n",
    "            if self.bigram:\n",
    "                bigrams = bigram(sentence)\n",
    "                for element in bigrams:\n",
    "                    if element not in self.words:\n",
    "                        self.words.append(element)\n",
    "            if self.trigram:\n",
    "                trigrams = trigram(sentence)\n",
    "                for element in trigrams:\n",
    "                    if element not in self.words:\n",
    "                        self.words.append(element)\n",
    "        #construire le vecteur des idfs\n",
    "        self.idf = []\n",
    "        for sentence in X :\n",
    "            sentence_idfs = []\n",
    "            for word in sentence : \n",
    "                if word in self.words:\n",
    "                    sentence_idfs.append(math.log(len(X)/self.get_occurence_in_document(X,word)))\n",
    "                else:\n",
    "                    sentence_idfs.append(0)\n",
    "            self.idf.append(sentence_idfs)\n",
    "\n",
    "        tf_idfs = self.computeTFIDF(X)\n",
    "        return tf_idfs\n",
    "                \n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.idf = []\n",
    "        for sentence in X :\n",
    "            sentence_idfs = []\n",
    "            for word in sentence : \n",
    "                if word in self.words:\n",
    "                    sentence_idfs.append(math.log(len(X)/self.get_occurence_in_document(X,word)))\n",
    "                else:\n",
    "                    sentence_idfs.append(0)\n",
    "            self.idf.append(sentence_idfs)\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
    "            \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        tf_idfs = self.computeTFIDF(X)\n",
    "        return tf_idfs\n",
    "        # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM-ZIw_Og4qb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis dans fit\n",
      "je commmence\n",
      "  (0, 0)\t0.08470725854916317\n",
      "  (0, 1)\t0.16941451709832633\n",
      "  (0, 2)\t0.05377111829115692\n",
      "  (0, 3)\t0.19290331197350633\n",
      "  (0, 4)\t0.16941451709832633\n",
      "  (0, 5)\t0.04822582799337658\n",
      "  (0, 6)\t0.08470725854916317\n",
      "  (0, 7)\t0.04822582799337658\n",
      "  (0, 8)\t0.08470725854916317\n",
      "  (0, 9)\t0.02688555914557846\n",
      "  (0, 10)\t0.08470725854916317\n",
      "  (0, 11)\t0.08470725854916317\n",
      "  (0, 12)\t0.08470725854916317\n",
      "  (1, 2)\t0.018919467546888544\n",
      "  (1, 13)\t0.03393669377311685\n",
      "  (1, 14)\t0.05960881157163334\n",
      "  (1, 15)\t0.05960881157163334\n",
      "  (1, 16)\t0.05960881157163334\n",
      "  (1, 17)\t0.05960881157163334\n",
      "  (1, 18)\t0.018919467546888544\n",
      "  (1, 19)\t0.05960881157163334\n",
      "  (1, 20)\t0.05960881157163334\n",
      "  (1, 21)\t0.11921762314326668\n",
      "  (1, 22)\t0.05960881157163334\n",
      "  (1, 23)\t0.03393669377311685\n",
      "  :\t:\n",
      "  (3, 54)\t0.064377516497364\n",
      "  (3, 55)\t0.064377516497364\n",
      "  (3, 56)\t0.064377516497364\n",
      "  (3, 57)\t0.064377516497364\n",
      "  (3, 58)\t0.064377516497364\n",
      "  (3, 59)\t0.064377516497364\n",
      "  (4, 3)\t0.1249487361646575\n",
      "  (4, 5)\t0.041649578721552505\n",
      "  (4, 9)\t0.02321934653481776\n",
      "  (4, 18)\t0.02321934653481776\n",
      "  (4, 29)\t0.08329915744310501\n",
      "  (4, 60)\t0.07315626874700457\n",
      "  (4, 61)\t0.07315626874700457\n",
      "  (4, 62)\t0.07315626874700457\n",
      "  (4, 63)\t0.07315626874700457\n",
      "  (4, 64)\t0.07315626874700457\n",
      "  (4, 65)\t0.07315626874700457\n",
      "  (4, 66)\t0.07315626874700457\n",
      "  (4, 67)\t0.07315626874700457\n",
      "  (4, 68)\t0.07315626874700457\n",
      "  (4, 69)\t0.07315626874700457\n",
      "  (4, 70)\t0.07315626874700457\n",
      "  (4, 71)\t0.07315626874700457\n",
      "  (4, 72)\t0.07315626874700457\n",
      "  (4, 73)\t0.07315626874700457\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "tfidbow = TFIDFBoW(pipeline)\n",
    "# print(train_X[:5])\n",
    "print(tfidbow.fit_transform(train_X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABWRnizRg4qf"
   },
   "source": [
    "## 7. Classification utilisant BoW\n",
    "\n",
    "Pour la classification, nous allons effectuer une régression logisitique (vu en cours ou que vous allez voir bientôt). \n",
    "Pour en savoir plus : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "La méthode `train_evaluate` entraîne et évalue le modèle de régression logistique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAlUVUVDg4qg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "    classifier = LogisticRegression(solver='liblinear',n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "    \n",
    "    print(\"bow object instancied\")\n",
    "    \n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQSGHWVZg4qj"
   },
   "source": [
    "\n",
    "##### Question 7. Entraînez et calculez la précision de la régression logistique sur les ensembles d'entraînement et de validation. (4 points)\n",
    "\n",
    "Essayez les configurations suivantes :\n",
    "\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "\n",
    "Outre la précision, reportez la taille du dictionnaire pour chacune des configurations. Enfin, décrivez vos résultats obtenus et répondez aux questions suivantes:\n",
    "- Quelles étapes de preprocessing ont effectivement aidé le modèle ? Pourquoi ?\n",
    "- La pondération avec TF-IDF a-t-elle aidé à obtenir une meilleure performance que le simple BoW ?\n",
    "- Les bigrams et trigrams ont-ils amélioré la performance ? Expliquez pourquoi.\n",
    "\n",
    "Indiquez quelle est la configuration que vous choisissez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "CCD69snJg4qk",
    "outputId": "7363f6b9-65bd-42f8-e88f-e70ebb176f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training for model 0\n",
      "je commence train\n",
      "je commence\n",
      "je fini\n",
      "bow object instancied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je commence\n",
      "je fini\n",
      "je finis train\n",
      "for  model 0 the train accuracy is: 0.9568796550372403 and the validation accuracy is :0.7915178571428572\n",
      "start training for model 1\n",
      "je commence train\n",
      "je commence\n",
      "je fini\n",
      "bow object instancied\n",
      "je commence\n",
      "je fini\n",
      "je finis train\n",
      "for  model 1 the train accuracy is: 0.9399255194041553 and the validation accuracy is :0.8053571428571429\n",
      "start training for model 2\n",
      "je commence train\n",
      "je suis dans fit\n",
      "je commmence\n",
      "bow object instancied\n",
      "je commmence\n",
      "je finis train\n",
      "for  model 2 the train accuracy is: 0.8529988239905919 and the validation accuracy is :0.6209821428571428\n",
      "start training for model 3\n",
      "je commence train\n",
      "je suis dans fit\n",
      "je commmence\n",
      "bow object instancied\n",
      "je commmence\n",
      "je finis train\n",
      "for  model 3 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n",
      "start training for model 4\n",
      "je commence train\n",
      "je suis dans fit\n",
      "je commmence\n",
      "bow object instancied\n",
      "je commmence\n",
      "je finis train\n",
      "for  model 4 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n",
      "start training for model 5\n",
      "je commence train\n",
      "je suis dans fit\n",
      "je commmence\n",
      "bow object instancied\n",
      "je commmence\n",
      "je finis train\n",
      "for  model 5 the train accuracy is: 0.8597608780870247 and the validation accuracy is :0.6200892857142857\n"
     ]
    }
   ],
   "source": [
    "#elements for configuration first:countbow, second: tokenization, third:twitter preprocessing, fourth:stemming,\n",
    "#fifth:bigram, six:trigram \n",
    "configurations = [[False,False,False,False,False,False],[False,True,False,False,False,False],[True,True,False,True,False,False],\\\n",
    "                 [True,True,True,True,False,False],[True,True,True,True,True,False],[True,True,True,True,True,True]]\n",
    "for configuration in configurations:\n",
    "        print(\"start training for model \" + str(configurations.index(configuration) + 1))\n",
    "        pipeline = PreprocessingPipeline(configuration[1], configuration[2], configuration[3])\n",
    "        if configuration[0] == True:\n",
    "            bowObj = TFIDFBoW(pipeline, configuration[4], configuration[5])\n",
    "        else:\n",
    "            bowObj = CountBoW(pipeline, configuration[4], configuration[5])\n",
    "\n",
    "        classifier,train_acc, validate_acc = train_evaluate(train_X,train_Y,valid_X,valid_Y,bowObj)\n",
    "        print (\"for  model \" + str(configurations.index(configuration)) + \" the train accuracy is: \" + str(train_acc) + \" and the validation accuracy is :\" + str(validate_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t77sO-8bg4qn"
   },
   "source": [
    "###### II/ Prototype (7 points)\n",
    "\n",
    "Maintenant que nous avons un modèle de classification entraîné pour l'analyse de sentiments, nous pouvons l'appliquer à notre ensemble de tests et analyser le résultat.\n",
    "\n",
    "## 1. Analyse de Sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFHFdRjcg4qo"
   },
   "source": [
    "##### Question 9. Implémentez la fonction `detect_airline` qui détecte la compagnie aérienne d'un tweet. (1,5 points)\n",
    "\n",
    "Expliquez votre approche, et les inconvénients possibles.\n",
    "\n",
    "**Attention :** `detect_airline` doit être en mesure de gérer le cas où aucune compagnie n'est mentionnée (auquel cas `None` est retounée), mais aussi le cas où plusieurs compagnies sont mentionnées dans un tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7UUdYFAg4qp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@americanair absolutely worst experience of my life at phl. 6 hrs, @jetblue 1 failed departure, 3 gates, and zero communication or help!!\n",
      "['JetBlue']\n"
     ]
    }
   ],
   "source": [
    "def detect_airline(tweet):\n",
    "    companies_term = [\"Virgin America\",\"United Airline\",\"Southwest Airlines\", \"JetBlue\", \"USAirways\", \"American Airlines\", \"VirginAmerica\",\"UnitedAirline\",\"SouthwestAirlines\", \"AmericanAirlines\", \"AmericanAir\", \"SouthwestAir\", \"southwest\", \"Southwest\", \"american\", \"American\"]\n",
    "\n",
    "    mentionned_companies = []\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    for term in companies_term:\n",
    "        if term.lower() in tweet.lower():\n",
    "            mentionned_companies.append(term)\n",
    "    return mentionned_companies\n",
    "    # TODO\n",
    "t ='@AmericanAir absolutely worst experience of my life at PHL. 6 hrs, @jetBlue 1 failed departure, 3 gates, and ZERO communication or help!!'\n",
    "d = detect_airline(t)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IopdH60Og4qs"
   },
   "source": [
    "\n",
    "##### Question 10. Implémentez la fonction `extract_sentiment` qui, à partir de tweets et d'un classificateur, extrait leurs sentiments. (0.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZ0_wKJwg4qt"
   },
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweets):\n",
    "    #On choisi la 2e configuration CountBoW + NLTKTokenizer + unigram\n",
    "    pipeline = PreprocessingPipeline(False, True, False)\n",
    "    countbow = CountBoW(pipeline)\n",
    "    dataset = countbow.fit_transform(tweets)\n",
    "    classifier.fit(dataset, train_Y)\n",
    "    test = classifier.predict(dataset)\n",
    "    print(test)\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je commence\n",
      "je fini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouns\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Mouns\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='liblinear',n_jobs=-1)\n",
    "extract_sentiment(classifier, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways tells me to talk to @AmericanAir about my delayed flights. AA tells me to talk to US. #ihatemergers\n",
      "['USAirways']\n",
      "@SouthwestAir Hi! I just saw a Black History month commercial on TV &amp; Im excited! In support of this month,will you all grant me 1 free trip\n",
      "[]\n",
      "@SouthwestAir Hoping you answer the phone today?\n",
      "[]\n",
      "@JetBlue not making a great first impression on my first flight. 20 minutes before boarding and the gate agent still can't assign me a seat?\n",
      "['JetBlue']\n",
      "@AmericanAir wasn't offered a flight out of PHL until TUESDAY so had to ask to be booked to Houston instead of Austin.\n",
      "[]\n",
      "@united what the hell? Flight 746. Delayed since 3 pm, finally boarded and now sitting on the tarmac? Is this f&amp;&amp;%$cking plane Ever leaving?\n",
      "[]\n",
      "@USAirways after missing my flight and reFlight Booking Problems 2x, I just walked onto another flight and my phone was still on the seat!!\n",
      "['USAirways']\n",
      "@AmericanAir Thx! I hope so. IAH to DFW to OKC has turned out to be a LONG trip today and I have to work tomorrow.\n",
      "[]\n",
      "@SouthwestAir broke the stroller my wife and baby gate checked. They told her it's not their problem. Calling the A List Preferred line now.\n",
      "[]\n",
      "@AmericanAir how is US4623 going to be on time when they are still deplaning at 4:08? This is BS.\n",
      "[]\n",
      "@USAirways My Flight Booking Problems C68LD9 just times out when I select it under Manage My Flight Booking Problems for months now. I have emailed but no response. Help?\n",
      "['USAirways']\n",
      "@JetBlue You guys really suck! I just spent 40 minutes on the phone linking my credits to my account.  They are still NOT there! 1/2\n",
      "['JetBlue']\n",
      "@AmericanAir check on what? Our broken tablet! See attached picture. #media #filmcrew #nbc #cnn http://t.co/Uq2ooPjPwg\n",
      "[]\n",
      "@united I'm confused. After your @Dulles_Airport agents directed us to go to your website after DELAYS nothing works! http://t.co/cypRHe5GOK\n",
      "[]\n",
      "@VirginAmerica has the most INCREDIBLE customer service I've ever experienced! So refreshing!\n",
      "['VirginAmerica']\n",
      "@AmericanAir You should be apologizing for your rude sales reps and failure to offer anything other than trite, condescending platitudes....\n",
      "[]\n",
      "@AmericanAir Aww Thanks AA..DFW was on GMA up here this AM..so i understand ..Btw A.A is my Airline when im able to trv..Love you guys.:)\n",
      "[]\n",
      "@AmericanAir 740pm wheels up to be exact.  I'm sending a DM.\n",
      "[]\n",
      "@AmericanAir not sure why we are being made to stand in line outside for plane that isn't ready to board when I could be sitting inside\n",
      "[]\n",
      "@AmericanAir My flight 1108 was Cancelled Flightled. I need to be rebooked, and I can't do it online and they aren't answering calls. Help!\n",
      "[]\n",
      "@USAirways So you understand why I'm resorting to Twitter: No help available at  gate while this occurred or by phone afterward.\n",
      "['USAirways']\n",
      "@AmericanAir @RobertDwyer AA doesnt charge any fees to change award tickets as long as the origin, destination &amp; award type remains the same\n",
      "[]\n",
      "@united it was very comfortable, now waiting for our luggage\n",
      "[]\n",
      "@AmericanAir @derekc21 , to my surprise  BA doesn't  have  me registered in the system , I was getting hopeful :(, Lets step it up , Okay ?\n",
      "[]\n",
      "@JetBlue flight attendant was hesitant to give me more baby sized cookies\n",
      "['JetBlue']\n",
      "@JetBlue you guys operate a world class company and for that I thank you\n",
      "['JetBlue']\n",
      "@AmericanAir Also, I have to wait more than 2 hours before I can speak to someone on the phone?  I can't wait 2 hours.  :(\n",
      "[]\n",
      "@VirginAmerica  for all my flight stuff wrong and did nothing about it. Had #worst #flight ever\n",
      "['VirginAmerica']\n",
      "@JetBlue @amybruni @DIRECTTV but of course! :-) #bestdressed #bluecarpet\n",
      "['JetBlue']\n",
      "@AmericanAir flight BA1551 departing tomorrow (wednesday) - not Cancelled Flightled but how can anyone make it to the airport in this weather?\n",
      "[]\n",
      "@SouthwestAir @Imaginedragons @beatsmusic well timed tweet, just boarded and will be listening on my way home!\n",
      "[]\n",
      "@united I have a 0530 flight out of DFW on Tuesday. How far in advance will you give notice if Cancelled Flightled?\n",
      "[]\n",
      "@AmericanAir been waiting on hold for more than an hour\n",
      "[]\n",
      "@JetBlue Not trying to make you look bad, on your website it says: \"Due to weather in the Charleston, NC\" Its actually in SC, not NC\n",
      "['JetBlue']\n",
      "@USAirways have checked all day.  CHO needs added.  Please evaluate.\n",
      "['USAirways']\n",
      "@united been solved,  they finally picked up the second time I called,  thanks for the response JH!  7:21 to dulles works!\n",
      "[]\n",
      "@SouthwestAir I'm excited too, but perhaps you could scale your excitement back by a few weeks...\n",
      "[]\n",
      "@SouthwestAir Maybe it is just a machine...\n",
      "[]\n",
      "@united nawww. United is my fave airline.\n",
      "[]\n",
      "@USAirways shout out to Cathy at the Vegas airport check-in for hooking us up!\n",
      "['USAirways']\n",
      "@united obviously the staff at EWR - United or otherwise have not seen your ads about - being FRIENDLY and helpful #ewr worst airport\n",
      "[]\n",
      "@AmericanAir 1 ticket agent servicing at least 60 people on line. Waiting for an hour and a half and your phones hang up on people.\n",
      "[]\n",
      "@AmericanAir thank you!\n",
      "[]\n",
      "@AmericanAir your call center won't let me wait on hold, which I would happily do. Am I seriously supposed to just keep calling? Not great\n",
      "[]\n",
      "@USAirways THANK YOU for resolving the issue. On direct flight to sfo from @united\n",
      "['USAirways']\n",
      "@AmericanAir AA 2258 why have you Cancelled Flightled our flight without courtesy of notifying us or alternatives?  Very poor.\n",
      "[]\n",
      "@united MIA-EWR #384 ???\"???\"???\" excellent crew. EWR-IAD #3589 ???!???!???! No crew to load bags - waiting w/ door open freezing. 20 mins past departure.\n",
      "[]\n",
      "@SouthwestAir booked our flights this morning. Can't wait to move about the country.\n",
      "[]\n",
      "@JetBlue so technically I could drive to JFK now and put in. Request for tomorrow's flight?\n",
      "['JetBlue']\n",
      "@SouthwestAir SW rocks, thanks for the reply and the follow. Rebooked earlier flight!\n",
      "[]\n",
      "@AmericanAir LAX-OGG-LAX using hard earned miles and was given lousy service, faulty seats on both legs and damaged bag. Your customer care\n",
      "[]\n",
      "@USAirways why I can't book a flight to PRN through your website?\n",
      "['USAirways']\n",
      "@united made to check in my carry on from Flight 1449 PBI to Newark. Plane luggage stow not full. U r costing me time, money &amp; aggravation!\n",
      "[]\n",
      "@USAirways  - I had 20,748 miles in  my account and was informed I'd forfeit them bc of 33 months of inactivity - I\n",
      "['USAirways']\n",
      "@united yr team have me on hold for over an hour unhelpful Pls can you answer my question.\n",
      "[]\n",
      "@united I've sent the message, let me know if you got it. I'm not very twitter-literate. Also, is he REALLY going to be stuck for 18hrs?\n",
      "[]\n",
      "@americanair Plz bring more agents up to DFW AA Cstmr srvc ctr gates A, there are only 2 agents and 100+ ppl... http://t.co/uCvEU2hurZ\n",
      "[]\n",
      "@united Ok thank again for your help!\n",
      "[]\n",
      "@AmericanAir refund\n",
      "[]\n",
      "@AmericanAir made it!  Thanks AA!\n",
      "[]\n",
      "@USAirways we called and were able to get rescheduled. Thank you for the quick responses today!!\n",
      "['USAirways']\n",
      "@JetBlue it will be glowing. Your crew and your aircraft sparkled. You guys know about @nokidhungry right? Might be a good partnership:)\n",
      "['JetBlue']\n",
      "@JetBlue not one of my four flights this trip has been on time. What the he'll is your companies problem?\n",
      "['JetBlue']\n",
      "@united thanks! We would like an apology and a full refund. Please confirm when we will receive.\n",
      "[]\n",
      "I left my dildo on the plane is there any way for me to get it back? @united\n",
      "[]\n",
      "@AmericanAir been calling for over 24hrs now and getting no where but told to call back by the automated system is extremely frustrating\n",
      "[]\n",
      "@SouthwestAir is hiring for their Emerging Leader Development Program- Last day to apply is TODAY! Apply here: http://t.co/jLHM0x7UkB\n",
      "[]\n",
      "@SouthwestAir it'd be nice if I could get an answer, even a simple \"no\" would do; I just want to know. :)\n",
      "[]\n",
      "@SouthwestAir DAL is due for sleet Sun. eve-didn't see it listed for cities that can re-book?  Fly to DCA at 8:10\n",
      "[]\n",
      "@JetBlue Airways Corporation (NASDAQ:JBLU) Reaches on New High Range ... - StreetWise Report http://t.co/C7tpdKqULM\n",
      "['JetBlue']\n",
      "@united You're trying to solve problem of your own making. Charging for checked luggage forces checking at gate. Brilliant.\n",
      "[]\n",
      "Nice RT @VirginAmerica: Vibe with the moodlight from takeoff to touchdown. #MoodlitMonday #ScienceBehindTheExperience http://t.co/Y7O0uNxTQP\n",
      "['VirginAmerica']\n",
      "@USAirways sitting on a plane in Philadelphia for over 20 minutes waiting just to get off the plane. Great service!\n",
      "['USAirways']\n",
      "@USAirways now the freezing bus is just doing circles?\n",
      "['USAirways']\n",
      "@AmericanAir no response to DM or email yet.  customer service?\n",
      "[]\n",
      "@SouthwestAir Ahhhh! Sorry, just followed.\n",
      "[]\n",
      "@SouthwestAir it's too damn cold to be sitting on this runway with no heat!!!!!\n",
      "[]\n",
      "@AmericanAir can I DM you info?\n",
      "[]\n",
      "@united Just saying the truth. You don't even an email! #2015\n",
      "[]\n",
      "@USAirways never imagined it would be so hard to SPEND money through US Air. Stubbornly hoping someone will come back to the phone.\n",
      "['USAirways']\n",
      "@JetBlue Do your TrueBlue pages not work on Chrome or Safari? I keep getting blank pages when I lick on the TrueBlue tab on your site.\n",
      "['JetBlue']\n",
      "@JetBlue My mother had overheard a personnel ask the attendant if \"the passengers really knew what was going on\" and the attendant said no.\n",
      "['JetBlue']\n",
      "@united still sucks. I don't understand why you have two cust service agents for a long line at Houston international b gate #unitedsucks\n",
      "[]\n",
      "@united someone should send a note to the revenue management team and ask about all the open BF and GF seats on 919.\n",
      "[]\n",
      "???@JetBlue: Our fleet's on fleek. http://t.co/cRFrwpc1Sx?????,???,???,\n",
      "['JetBlue']\n",
      "@united  delayed about 8 hours because of missed connections due to mechanical issues on 1st flight. rebooked, but please call me 9148445695\n",
      "[]\n",
      "@USAirways is anyone working today?? Anyone want to pick up a phone?\n",
      "['USAirways']\n",
      "@SouthwestAir Yes I can. But you guys should get your act together.\n",
      "[]\n",
      "@JetBlue Will do.  I'll tell him you say hey.\n",
      "['JetBlue']\n",
      "@USAirways TYVM USAir, Happy Night@ &lt;3\n",
      "['USAirways']\n",
      "@USAirways thanks to Betty working gate at ILM and lovely gate agents here in CLT helping me get home 2 Phx tonight instead of tomorrow\n",
      "['USAirways']\n",
      "@united unavailable leg that registered hours after being sold out. Only option to offer after 45min was +$400/ticket or Travelocity (2of2)\n",
      "[]\n",
      "@USAirways $75 for that uncertainty? I know u run an account\n",
      "['USAirways']\n",
      "@AmericanAir yea no worries. I'm just going home to Denver. Not your fault, weather sucks bad.\n",
      "[]\n",
      "@SouthwestAir same here. Would appreciate a follow so I can DM my info to figure out what I am supposed to do.\n",
      "[]\n",
      "@united if you're listening, why are you the lowest provider of inflight wifi? Get with the times!\n",
      "[]\n",
      "@USAirways u would think if u were going to Cancelled Flight my flight, you'd rebook our seats to something other than the last row. #usairwaysfail\n",
      "['USAirways']\n",
      "@VirginAmerica has getaway deals through May, from $59 one-way. Lots of cool cities http://t.co/tZZJhuIbCH #CheapFlights #FareCompare\n",
      "['VirginAmerica']\n",
      "@AmericanAir the group of minor children were broken up into groups. Some still in Miami. Delta has much better customer service!\n",
      "[]\n",
      "@USAirways Going to Manchester, UK now i'm going to miss a wedding. You going to rebook that too? Why have you chosen to Cancelled Flight my flight??\n",
      "['USAirways']\n"
     ]
    }
   ],
   "source": [
    "for x in train_X[:100]:\n",
    "    print(x)\n",
    "    print(detect_airline(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMEG4m6lg4qw"
   },
   "source": [
    "##### Question 11. En utilisant `extract_tweet_content`, `detect_airline` et `extract_sentiment`, générez un diagramme en bar contenant le nombre de tweets positives, négatifs et neutres pour chacune des compagnies. (2 points)\n",
    "\n",
    "Décrivez brièvement le diagramme et analysez les résultats (par exemple, quelle est la compagnie avec le plus de tweets négatifs?). Expliquez comment un tel diagramme peut aider des compagnies aériennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE77N5R3g4qx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"@AmericanAir\" in \"AmericanAirlines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMdg84mKg4qz"
   },
   "source": [
    "## 2. Analyse de termes\n",
    "\n",
    "Le POS-tagging (pour *part-of-speech tagging*, en français étiquetage grammatical) consiste à l'extraction de l'information grammaticale d'un token dans une phrase. Par exemple, la table ci-dessous donne un exemple du *POS-tagging* de la phrase *\"The cat is white!\"*\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "Pour autant, le *POS-tagging* peut être plus complexe que les règles simples apprises à l'école. Il faut souvent des informations plus détaillées sur le rôle d'un terme dans une phrase. Pour notre problème, nous n'avons pas besoin d'utiliser un modèle linguistique plus complexe, nous allons utiliser ce qu'on appelle des *POS-tags* universelles.\n",
    "\n",
    "En *POS-tagging*, chaque token est représenté par un tag. La liste des POS-tags utilisés sont disponibles ici :\n",
    "https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7Spx4Mag4q0"
   },
   "outputs": [],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBXeIH7Ug4q2"
   },
   "source": [
    "##### Question 12. Implémentez un code qui collecte les 10 termes les plus fréquents pour chaque compagnie aérienne. (2 Pts)\n",
    "\n",
    "Ici, vous n'allez considérer que les termes apparaissant dans les tweets positifs et négatifs. \n",
    "\n",
    "De plus, nous allons utiliser la définition suivante de \"terme\":\n",
    "\n",
    "1. Un mot qui est soit un adjectif, soit un nom\n",
    "2. Un N-gram composé d'adjectifs suivit par un nom (par exemple, \"nice place\"), ou un nom suivi par un autre nom (par exemple, \"sports club\").\n",
    "\n",
    "Ensuite, **générez une table** contenant les 10 termes les plus fréquents, avec leurs fréquences (en pourcentage) pour chaque compagnie.\n",
    "\n",
    "*N'oubliez pas de supprimer le nom de la compagnie parmi les termes !*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zuuk_9Qg4q4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCuICldzg4q7"
   },
   "source": [
    "##### Question 13. Que conclure de la table généré à la question 12 pour les compagnies ? (1 Pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbiNzWueg4q8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sFCAAccg4q-"
   },
   "source": [
    "# III/ Bonus (2 points)\n",
    "\n",
    "Les noms de personnes, les noms de sociétés et les emplacements sont appelés \"entités nommées\". La reconnaissance d'entité nommée (NER, pour *Named-entity recognition*) consiste à extraire les entités nommées en les classant à l'aide de catégories prédéfinies. Dans cette section bonus, vous utiliserez un outil de NER pour extraire automatiquement des entités nommées des tweets. Cette approche est suffisamment générique pour récupérer des informations sur d’autres sociétés ou même des noms de produits et de personnes.\n",
    "\n",
    "\n",
    "**Pour le bonus, vous êtes libres d'utiliser n'importe quel NER implémenté en Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwmEecqQg4q-"
   },
   "source": [
    "##### Question Bonus 1.  Implémentez un code qui génère une table contenant le top 10 des NER de la base de données. (1 point)\n",
    "\n",
    "Cette table doit contenir les fréquences des entités nommées. Ensuite, générez un diagramme en bar qui montre le nombre de tweets positifs, négatifs ou neutres pour chacunes des 10 NER. Décrivez le résultat obtenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8V5jAWjg4q_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToIUiL6Ug4rB"
   },
   "source": [
    "##### Question Bonus 2. Générez une table similaire à la question 12 pour le top 10 des NER pour chaque compagnie. (1 point)\n",
    "\n",
    "Que peut-on conclure de ces résultats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wyf05Kcvg4rC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP1_INF8111_2019.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
