{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 Automne 2019 - Preprocessing de tweets pour de l'analyse de sentiments\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation\n",
    "\n",
    "Twitter est un réseau social permettant aux utilisateurs de publier des informations et communiquer entre eux par le biais de messages, appelés tweets, pouvant contenir jusqu'à 280 caractères. Largement utilisé aujourd'hui, ce réseau peut être un outil pour des entreprises qui souhaitent évaluer l'avis de leurs clients.\n",
    "\n",
    "Dans ce TP, on se met à la place d'une compagnie aérienne, qui souhaiterait détecter les tweets qui la mentionnent et analyser si ce sont des mentions positives ou négatives, en comparant leur résultat avec les autres compagnies.\n",
    "\n",
    "Le *preprocessing* est une tâche cruciale en fouille de données. Elle permet de transformer les données brutes en un format adapté à l'application de méthodes de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# I/ Analyse de sentiments (13 Pts)\n",
    "\n",
    "Usuellement dans la littérature, la tâche d'extraire le sentiment d'un texte est appelé *sentiment analysis*.\n",
    "Ici pour se faire, nous allons utiliser un modèle *bag-of-words* (BoW).\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
    "\n",
    "Installez les libraires en question et exécutez le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# !pip install --user numpy\n",
    "# !pip install --user sklearn\n",
    "# !pip install --user scipy\n",
    "# !pip install --user nltk\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"universal_tagset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jeu de données\n",
    "\n",
    "On utilise un jeu de donnée provenant de *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Pour citer la source originale de la base :\n",
    "\n",
    "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "Les compagnies incluses dans cette base de données sont Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, et American Airlines.\n",
    "\n",
    "Dans le fichier zip du TP, vous trouverez le fichier *airline_tweets_database.csv*, qui est la base de données de tweets que nous allons manipuler.\n",
    "\n",
    "Chaque ligne de ce fichier contient un tweet, avec plusieurs informations : l'identifiant du tweet, l'utilisateur, le contenu, le nombre de retweet... Ainsi que le label.\n",
    "\n",
    "3 labels différents sont possibles dans ce dataset : *négatif*, *neutre* et *positif*, que l'on va représenter respectivement par 0, 1 et 2.\n",
    "\n",
    "Pour ce TP, on ne va conserver ici que le texte et le label. On divise ensuite la base de données en 3 ensembles (entrainement/validation/test). Vous utiliserez l'ensemble d'entraînement et de validation pour cette partie, et l'ensemble de test à la partie suivante.\n",
    "\n",
    "Le code ci-dessous permet de charger ces ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  10204\n",
      "Length of validation set :  2240\n",
      "Length of test set :  2196\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('airline_sentiment')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            \n",
    "            if label == \"negative\":\n",
    "                y.append(0)\n",
    "            elif label == \"neutral\":\n",
    "                y.append(1)\n",
    "            elif label == \"positive\":\n",
    "                y.append(2)\n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"data/airline_tweets_database.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Nous allons ici implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP. Ensuite, afin d'avoir un modèle qui s'adapte mieux au format de Twitter, nous ajouterons une étape spécifique supplémentaire.\n",
    "\n",
    "### 3.1. Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
    "\n",
    "\n",
    "##### Question 1. Implémentez les 2 tokenizers différents suivants: (0.5 Pts)\n",
    "\n",
    "- Le **SpaceTokenizer**, qui est un tokenizer naïf qui sépare simplement en fonction des espaces.\n",
    "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # TODO\n",
    "        tokens = []\n",
    "        tokens = text.split()\n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # TODO\n",
    "        tokens = TweetTokenizer().tokenize(text)\n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testez les deux tokenizers. Quelles différences pouvez-vous constater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "chaine = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "nltkTokenizer = NLTKTokenizer()\n",
    "spaceTokenizer = SpaceTokenizer()\n",
    "print(nltkTokenizer.tokenize(chaine))\n",
    "print(spaceTokenizer.tokenize(chaine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons constater plus haut que le tweetTokenizer fait une distinction avec les ponctuations et autres symboles alors que le spaceTokenizer n'en fait pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Troncature (ou Stemming)\n",
    "\n",
    "Dans les phrases \"I should have bought a new shoes today\" et \"I spent too much money buying games\", les mots \"buying\" et \"bought\" représentent la même idée. Considérer ces deux mots comme différents ne ferait qu'augmenter pour rien la complexité et la dimension du problème, ce qui peut avoir un impact négatif sur la performance globale. Ainsi, on peut donc plutôt une forme unique (comme la racine du mot) pour représenter ces deux mots de la même manière. Ce procédé de conversion de mots en racines permettant de réduire la dimension est appelé usuellement *stemming*, que l'on peut traduire par troncature.\n",
    "\n",
    "\n",
    "#### Question 2. Récupérez les troncatures des tokens en utilisant l'attribut *stemmer* de la classe *Stemmer* (0.5 Pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, token):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        stem = self.stemmer.stem(token)\n",
    "        \n",
    "        # Have to return a list of stems\n",
    "        return stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm\n"
     ]
    }
   ],
   "source": [
    "#for testing by Marc\n",
    "input_str = \"algorithms\"\n",
    "stemmer = Stemmer()\n",
    "stem_output = stemmer.stem(input_str)\n",
    "print (stem_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Twitter preprocessing\n",
    "\n",
    "Parfois, appliquer uniquement ces deux étapes ne suffit pas, due aux particularités des données que nous manipulons, qui peuvent demander des étapes de preprocessing spécifiques afin d'obtenir un modèle plus adapté.\n",
    "\n",
    "Couramment en NLP, un dictionnaire est utilisé pour stocker un ensemble de mots, et tous les mots n'appartenant pas au dictionnaire sont considérés comme inconnus. Ainsi, avec ce choix d'implémentation, la dimension de l'espace caractéristique du modèle est directement liée au nombre de mots du dictionnaire. Ainsi, pour des raisons de complexité mais aussi car les modèles à trop grande dimension peuvent souffrir du fléau de la dimensionnalité, il est préférable de réduire la taille de notre vocabulaire.\n",
    "\n",
    "#### Question 3. Donnez, en expliquant brièvement, au moins deux exemples d'étapes de préprocessing qui permettent de réduire la taille du dictionnaire ici, puis implémentez-les.  (2.0 points)\n",
    "\n",
    "Ces étapes de préprocessing doivent être en rapport aux charactéristiques spécifiques des données de Twitter. La suppression des mots vides ne compte pas comme une des deux étapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        # TODO : Write your preprocessing steps here.\n",
    "        #remove punctuations, numbers , special characters\n",
    "        #remove URls            \n",
    "        for w in tweet: \n",
    "            if w in string.punctuation: \n",
    "                tweet.remove(w)\n",
    "            if(re.match(r\"http*\", w)):\n",
    "                tweet.remove(w)\n",
    "        new_tweet = tweet\n",
    "        # return the preprocessed twitter\n",
    "        return new_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.  Pipeline\n",
    "\n",
    "Une pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer, les troncatures et le preprocessing spécifique à Twitter. \n",
    "\n",
    "**N'hésitez pas à changer l'ordre des étapes de preprocessing si vous le souhaitez.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "        print(tweet)\n",
    "        if self.stemmer:\n",
    "            tokens = list(map(self.stemmer.stem, tokens))\n",
    "        if self.twitterPreprocesser:\n",
    "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
    "        print('mes tokens \\n', tokens)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways tells me to talk to @AmericanAir about my delayed flights. AA tells me to talk to US. #ihatemergers\n",
      "mes tokens \n",
      " ['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg']\n",
      "@SouthwestAir Hi! I just saw a Black History month commercial on TV &amp; Im excited! In support of this month,will you all grant me 1 free trip\n",
      "mes tokens \n",
      " ['@southwestair', 'hi', 'i', 'just', 'saw', 'a', 'black', 'histori', 'month', 'commerci', 'on', 'tv', 'im', 'excit', 'in', 'support', 'of', 'this', 'month', 'will', 'you', 'all', 'grant', 'me', '1', 'free', 'trip']\n",
      "@SouthwestAir Hoping you answer the phone today?\n",
      "mes tokens \n",
      " ['@southwestair', 'hope', 'you', 'answer', 'the', 'phone', 'today']\n",
      "@JetBlue not making a great first impression on my first flight. 20 minutes before boarding and the gate agent still can't assign me a seat?\n",
      "mes tokens \n",
      " ['@jetblu', 'not', 'make', 'a', 'great', 'first', 'impress', 'on', 'my', 'first', 'flight', '20', 'minut', 'before', 'board', 'and', 'the', 'gate', 'agent', 'still', \"can't\", 'assign', 'me', 'a', 'seat']\n",
      "@AmericanAir wasn't offered a flight out of PHL until TUESDAY so had to ask to be booked to Houston instead of Austin.\n",
      "mes tokens \n",
      " ['@americanair', \"wasn't\", 'offer', 'a', 'flight', 'out', 'of', 'phl', 'until', 'tuesday', 'so', 'had', 'to', 'ask', 'to', 'be', 'book', 'to', 'houston', 'instead', 'of', 'austin']\n",
      "@united what the hell? Flight 746. Delayed since 3 pm, finally boarded and now sitting on the tarmac? Is this f&amp;&amp;%$cking plane Ever leaving?\n",
      "mes tokens \n",
      " ['@unit', 'what', 'the', 'hell', 'flight', '746', 'delay', 'sinc', '3', 'pm', 'final', 'board', 'and', 'now', 'sit', 'on', 'the', 'tarmac', 'is', 'this', 'f', '&', '$', 'cking', 'plane', 'ever', 'leav']\n",
      "@USAirways after missing my flight and reFlight Booking Problems 2x, I just walked onto another flight and my phone was still on the seat!!\n",
      "mes tokens \n",
      " ['@usairway', 'after', 'miss', 'my', 'flight', 'and', 'reflight', 'book', 'problem', '2x', 'i', 'just', 'walk', 'onto', 'anoth', 'flight', 'and', 'my', 'phone', 'was', 'still', 'on', 'the', 'seat', '!']\n",
      "@AmericanAir Thx! I hope so. IAH to DFW to OKC has turned out to be a LONG trip today and I have to work tomorrow.\n",
      "mes tokens \n",
      " ['@americanair', 'thx', 'i', 'hope', 'so', 'iah', 'to', 'dfw', 'to', 'okc', 'has', 'turn', 'out', 'to', 'be', 'a', 'long', 'trip', 'today', 'and', 'i', 'have', 'to', 'work', 'tomorrow']\n",
      "@SouthwestAir broke the stroller my wife and baby gate checked. They told her it's not their problem. Calling the A List Preferred line now.\n",
      "mes tokens \n",
      " ['@southwestair', 'broke', 'the', 'stroller', 'my', 'wife', 'and', 'babi', 'gate', 'check', 'they', 'told', 'her', \"it's\", 'not', 'their', 'problem', 'call', 'the', 'a', 'list', 'prefer', 'line', 'now']\n",
      "@AmericanAir how is US4623 going to be on time when they are still deplaning at 4:08? This is BS.\n",
      "mes tokens \n",
      " ['@americanair', 'how', 'is', 'us4623', 'go', 'to', 'be', 'on', 'time', 'when', 'they', 'are', 'still', 'deplan', 'at', '4:08', 'this', 'is', 'bs']\n",
      "@USAirways My Flight Booking Problems C68LD9 just times out when I select it under Manage My Flight Booking Problems for months now. I have emailed but no response. Help?\n",
      "mes tokens \n",
      " ['@usairway', 'my', 'flight', 'book', 'problem', 'c68ld9', 'just', 'time', 'out', 'when', 'i', 'select', 'it', 'under', 'manag', 'my', 'flight', 'book', 'problem', 'for', 'month', 'now', 'i', 'have', 'email', 'but', 'no', 'respons', 'help']\n",
      "@JetBlue You guys really suck! I just spent 40 minutes on the phone linking my credits to my account.  They are still NOT there! 1/2\n",
      "mes tokens \n",
      " ['@jetblu', 'you', 'guy', 'realli', 'suck', 'i', 'just', 'spent', '40', 'minut', 'on', 'the', 'phone', 'link', 'my', 'credit', 'to', 'my', 'account', 'they', 'are', 'still', 'not', 'there', '1/2']\n",
      "@AmericanAir check on what? Our broken tablet! See attached picture. #media #filmcrew #nbc #cnn http://t.co/Uq2ooPjPwg\n",
      "URLLLLLL  http://t.co/uq2oopjpwg\n",
      "mes tokens \n",
      " ['@americanair', 'check', 'on', 'what', 'our', 'broken', 'tablet', 'see', 'attach', 'pictur', '#media', '#filmcrew', '#nbc', '#cnn']\n",
      "@united I'm confused. After your @Dulles_Airport agents directed us to go to your website after DELAYS nothing works! http://t.co/cypRHe5GOK\n",
      "URLLLLLL  http://t.co/cyprhe5gok\n",
      "mes tokens \n",
      " ['@unit', \"i'm\", 'confus', 'after', 'your', '@dulles_airport', 'agent', 'direct', 'us', 'to', 'go', 'to', 'your', 'websit', 'after', 'delay', 'noth', 'work']\n",
      "@VirginAmerica has the most INCREDIBLE customer service I've ever experienced! So refreshing!\n",
      "mes tokens \n",
      " ['@virginamerica', 'has', 'the', 'most', 'incred', 'custom', 'servic', \"i'v\", 'ever', 'experienc', 'so', 'refresh']\n",
      "@AmericanAir You should be apologizing for your rude sales reps and failure to offer anything other than trite, condescending platitudes....\n",
      "mes tokens \n",
      " ['@americanair', 'you', 'should', 'be', 'apolog', 'for', 'your', 'rude', 'sale', 'rep', 'and', 'failur', 'to', 'offer', 'anyth', 'other', 'than', 'trite', 'condescend', 'platitud', '...']\n",
      "@AmericanAir Aww Thanks AA..DFW was on GMA up here this AM..so i understand ..Btw A.A is my Airline when im able to trv..Love you guys.:)\n",
      "mes tokens \n",
      " ['@americanair', 'aww', 'thank', 'aa', '..', 'dfw', 'was', 'on', 'gma', 'up', 'here', 'this', 'am', '..', 'so', 'i', 'understand', '..', 'btw', 'a', 'a', 'is', 'my', 'airlin', 'when', 'im', 'abl', 'to', 'trv', '..', 'love', 'you', 'guy', ':)']\n",
      "@AmericanAir 740pm wheels up to be exact.  I'm sending a DM.\n",
      "mes tokens \n",
      " ['@americanair', '740pm', 'wheel', 'up', 'to', 'be', 'exact', \"i'm\", 'send', 'a', 'dm']\n",
      "[['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg'], ['@southwestair', 'hi', 'i', 'just', 'saw', 'a', 'black', 'histori', 'month', 'commerci', 'on', 'tv', 'im', 'excit', 'in', 'support', 'of', 'this', 'month', 'will', 'you', 'all', 'grant', 'me', '1', 'free', 'trip'], ['@southwestair', 'hope', 'you', 'answer', 'the', 'phone', 'today'], ['@jetblu', 'not', 'make', 'a', 'great', 'first', 'impress', 'on', 'my', 'first', 'flight', '20', 'minut', 'before', 'board', 'and', 'the', 'gate', 'agent', 'still', \"can't\", 'assign', 'me', 'a', 'seat'], ['@americanair', \"wasn't\", 'offer', 'a', 'flight', 'out', 'of', 'phl', 'until', 'tuesday', 'so', 'had', 'to', 'ask', 'to', 'be', 'book', 'to', 'houston', 'instead', 'of', 'austin'], ['@unit', 'what', 'the', 'hell', 'flight', '746', 'delay', 'sinc', '3', 'pm', 'final', 'board', 'and', 'now', 'sit', 'on', 'the', 'tarmac', 'is', 'this', 'f', '&', '$', 'cking', 'plane', 'ever', 'leav'], ['@usairway', 'after', 'miss', 'my', 'flight', 'and', 'reflight', 'book', 'problem', '2x', 'i', 'just', 'walk', 'onto', 'anoth', 'flight', 'and', 'my', 'phone', 'was', 'still', 'on', 'the', 'seat', '!'], ['@americanair', 'thx', 'i', 'hope', 'so', 'iah', 'to', 'dfw', 'to', 'okc', 'has', 'turn', 'out', 'to', 'be', 'a', 'long', 'trip', 'today', 'and', 'i', 'have', 'to', 'work', 'tomorrow'], ['@southwestair', 'broke', 'the', 'stroller', 'my', 'wife', 'and', 'babi', 'gate', 'check', 'they', 'told', 'her', \"it's\", 'not', 'their', 'problem', 'call', 'the', 'a', 'list', 'prefer', 'line', 'now'], ['@americanair', 'how', 'is', 'us4623', 'go', 'to', 'be', 'on', 'time', 'when', 'they', 'are', 'still', 'deplan', 'at', '4:08', 'this', 'is', 'bs'], ['@usairway', 'my', 'flight', 'book', 'problem', 'c68ld9', 'just', 'time', 'out', 'when', 'i', 'select', 'it', 'under', 'manag', 'my', 'flight', 'book', 'problem', 'for', 'month', 'now', 'i', 'have', 'email', 'but', 'no', 'respons', 'help'], ['@jetblu', 'you', 'guy', 'realli', 'suck', 'i', 'just', 'spent', '40', 'minut', 'on', 'the', 'phone', 'link', 'my', 'credit', 'to', 'my', 'account', 'they', 'are', 'still', 'not', 'there', '1/2'], ['@americanair', 'check', 'on', 'what', 'our', 'broken', 'tablet', 'see', 'attach', 'pictur', '#media', '#filmcrew', '#nbc', '#cnn'], ['@unit', \"i'm\", 'confus', 'after', 'your', '@dulles_airport', 'agent', 'direct', 'us', 'to', 'go', 'to', 'your', 'websit', 'after', 'delay', 'noth', 'work'], ['@virginamerica', 'has', 'the', 'most', 'incred', 'custom', 'servic', \"i'v\", 'ever', 'experienc', 'so', 'refresh'], ['@americanair', 'you', 'should', 'be', 'apolog', 'for', 'your', 'rude', 'sale', 'rep', 'and', 'failur', 'to', 'offer', 'anyth', 'other', 'than', 'trite', 'condescend', 'platitud', '...'], ['@americanair', 'aww', 'thank', 'aa', '..', 'dfw', 'was', 'on', 'gma', 'up', 'here', 'this', 'am', '..', 'so', 'i', 'understand', '..', 'btw', 'a', 'a', 'is', 'my', 'airlin', 'when', 'im', 'abl', 'to', 'trv', '..', 'love', 'you', 'guy', ':)'], ['@americanair', '740pm', 'wheel', 'up', 'to', 'be', 'exact', \"i'm\", 'send', 'a', 'dm']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "print(list(map(pipeline.preprocess, train_X[:18])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N-grams\n",
    "\n",
    "Un n-gram est une séquence continue de *n* tokens dans un texte. Par exemple, les séquences *\"nous a\"* et *\"la porte\"* sont deux exemples de 2-grams de la phrase *\"Il nous a dit au revoir en franchissant la porte.\"*. 1-gram, 2-gram et 3-gram sont respectivement appelés unigram, bigram et trigram. \n",
    "\n",
    "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
    "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
    "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n",
    "\n",
    "\n",
    "##### Question 4. Implementez les fonctions `bigram` et `trigram`. (1 Pt)\n",
    "\n",
    "Vous devez résoudre cette question sans utiliser de libraire exterieur comme scikit-learn par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ngrams = zip(*[tokens[i:] for i in range(2)])\n",
    "    bigrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ngrams = zip(*[tokens[i:] for i in range(3)])\n",
    "    trigrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "    # This function returns the list of trigrams\n",
    "    return trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
      "['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "print(bigram(tokens))\n",
    "print(trigram(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bag-of-words\n",
    "\n",
    "Régressions logistiques, SVM et d'autres modèles très courants demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Pandemic is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<i></i>     | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n",
    "\n",
    "##### Question 5. Implémentez le Bag-of-Words  (2 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "    #A changer \n",
    "    def computeBoW(self, X):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for tweet in X:\n",
    "            for word in self.words:\n",
    "                count = 0\n",
    "                if word in tweet:\n",
    "                    count = count + 1\n",
    "                    rows.append(X.index(tweet))\n",
    "                    cols.append(self.words.index(word))\n",
    "                    data.append(count)\n",
    "        csr_matrix = csr_matrix((data, (row, col)), shape=(len(X), len(self.words)))\n",
    "                \n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "        \n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        \n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        #construire le dictionnaire \n",
    "        self.words = []\n",
    "        for sentence in X : \n",
    "            for word in sentence :\n",
    "                self.words.append(word)\n",
    "            if self.bigram:\n",
    "                bigrams = bigram(sentence)\n",
    "                for element in bigrams:\n",
    "                    self.words.append(element)\n",
    "            if self.trigram:\n",
    "                trigrams = trigram(sentence)\n",
    "                for element in trigrams:\n",
    "                    self.words.append(element)\n",
    "        return self.computeBOW()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
    "\n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de tweets de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des tweets sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
    "\n",
    "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
    "\\end{equation}\n",
    "\n",
    "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Question 6. Implémentez le bag-of-words avec la pondération de TF-IDF (3 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        idf: list of idfs for each document\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "    def get_occurence_in_document(self,X,word):\n",
    "        count = 0\n",
    "        for sentence in X:\n",
    "            if word in sentence:\n",
    "                count = count +1\n",
    "        return count        \n",
    "     \n",
    "    def get_frequence_word_in_sentence(self, sentence, word):\n",
    "        return sentence.count(word)/len(sentence)\n",
    "        \n",
    "    \n",
    "    def computeTFIDF(self, X): \n",
    "        cs_matrix = None\n",
    "        \"\"\"\n",
    "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
    "        liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
    "        calculé le vecteur contenant l'idf pour chaque document.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for sentence in X :\n",
    "            sentence_idfs = self.idf[X.index(sentence)]\n",
    "            for word in self.words:\n",
    "                if word in sentence:\n",
    "                    tdidf = self.get_frequence_word_in_sentence(sentence,word) * sentence_idfs[sentence.index(word)]\n",
    "                    rows.append(X.index(sentence))\n",
    "                    cols.append(self.words.index(word))\n",
    "                    data.append(tdidf)\n",
    "                    \n",
    "        cs_matrix = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.words)))\n",
    "        return cs_matrix\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        #construire le dictionnaire \n",
    "        self.words = []\n",
    "        for sentence in X : \n",
    "            for word in sentence :\n",
    "                self.words.append(word)\n",
    "            if self.bigram:\n",
    "                bigrams = bigram(sentence)\n",
    "                for element in bigrams:\n",
    "                    self.words.append(element)\n",
    "            if self.trigram:\n",
    "                trigrams = trigram(sentence)\n",
    "                for element in trigrams:\n",
    "                    self.words.append(element)\n",
    "                    \n",
    "        #construire le vecteur des idfs\n",
    "        self.idf = []\n",
    "        for sentence in X :\n",
    "            sentence_idfs = []\n",
    "            for word in sentence : \n",
    "                if word in self.words:\n",
    "                    sentence_idfs.append(math.log(len(X)/self.get_occurence_in_document(X,word)))\n",
    "                else:\n",
    "                    sentence_idfs\n",
    "            self.idf.append(sentence_idfs)\n",
    "        tf_idfs = self.computeTFIDF(X)\n",
    "        return tf_idfs\n",
    "                \n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = list(map(pipeline.preprocess, X))\n",
    "        for sentence in X : \n",
    "            if self.bigram:\n",
    "                bigrams = bigram(sentence)\n",
    "                for element in bigrams:\n",
    "                    self.words.append(element)\n",
    "            if self.trigram:\n",
    "                trigrams = trigram(sentence)\n",
    "                for element in trigrams:\n",
    "                    self.words.append(element)\n",
    "\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
    "            \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        tf_idfs = computeTFIDF(X)\n",
    "        return tf_idfs\n",
    "        # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways tells me to talk to @AmericanAir about my delayed flights. AA tells me to talk to US. #ihatemergers\n",
      "mes tokens \n",
      " ['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg']\n",
      "@SouthwestAir Hi! I just saw a Black History month commercial on TV &amp; Im excited! In support of this month,will you all grant me 1 free trip\n",
      "mes tokens \n",
      " ['@southwestair', 'hi', 'i', 'just', 'saw', 'a', 'black', 'histori', 'month', 'commerci', 'on', 'tv', 'im', 'excit', 'in', 'support', 'of', 'this', 'month', 'will', 'you', 'all', 'grant', 'me', '1', 'free', 'trip']\n",
      "@SouthwestAir Hoping you answer the phone today?\n",
      "mes tokens \n",
      " ['@southwestair', 'hope', 'you', 'answer', 'the', 'phone', 'today']\n",
      "@JetBlue not making a great first impression on my first flight. 20 minutes before boarding and the gate agent still can't assign me a seat?\n",
      "mes tokens \n",
      " ['@jetblu', 'not', 'make', 'a', 'great', 'first', 'impress', 'on', 'my', 'first', 'flight', '20', 'minut', 'before', 'board', 'and', 'the', 'gate', 'agent', 'still', \"can't\", 'assign', 'me', 'a', 'seat']\n",
      "@AmericanAir wasn't offered a flight out of PHL until TUESDAY so had to ask to be booked to Houston instead of Austin.\n",
      "mes tokens \n",
      " ['@americanair', \"wasn't\", 'offer', 'a', 'flight', 'out', 'of', 'phl', 'until', 'tuesday', 'so', 'had', 'to', 'ask', 'to', 'be', 'book', 'to', 'houston', 'instead', 'of', 'austin']\n",
      "  (0, 0)\t0.08470725854916317\n",
      "  (0, 1)\t0.33882903419665267\n",
      "  (0, 2)\t0.21508447316462767\n",
      "  (0, 3)\t1.3503231838145444\n",
      "  (0, 4)\t0.33882903419665267\n",
      "  (0, 6)\t0.09645165598675316\n",
      "  (0, 7)\t0.08470725854916317\n",
      "  (0, 8)\t0.09645165598675316\n",
      "  (0, 9)\t0.08470725854916317\n",
      "  (0, 10)\t0.08065667743673538\n",
      "  (0, 11)\t0.08470725854916317\n",
      "  (0, 17)\t0.08470725854916317\n",
      "  (0, 18)\t0.08470725854916317\n",
      "  (1, 2)\t0.07567787018755417\n",
      "  (1, 19)\t0.0678733875462337\n",
      "  (1, 20)\t0.05960881157163334\n",
      "  (1, 21)\t0.05960881157163334\n",
      "  (1, 22)\t0.05960881157163334\n",
      "  (1, 23)\t0.05960881157163334\n",
      "  (1, 24)\t0.07567787018755417\n",
      "  (1, 25)\t0.05960881157163334\n",
      "  (1, 26)\t0.05960881157163334\n",
      "  (1, 27)\t0.23843524628653337\n",
      "  (1, 28)\t0.05960881157163334\n",
      "  (1, 29)\t0.0678733875462337\n",
      "  :\t:\n",
      "  (3, 70)\t0.064377516497364\n",
      "  (3, 71)\t0.064377516497364\n",
      "  (3, 72)\t0.064377516497364\n",
      "  (3, 73)\t0.064377516497364\n",
      "  (3, 74)\t0.064377516497364\n",
      "  (3, 77)\t0.064377516497364\n",
      "  (4, 3)\t0.8746411531526025\n",
      "  (4, 6)\t0.08329915744310501\n",
      "  (4, 10)\t0.06965803960445328\n",
      "  (4, 24)\t0.09287738613927105\n",
      "  (4, 35)\t0.24989747232931503\n",
      "  (4, 79)\t0.07315626874700457\n",
      "  (4, 80)\t0.07315626874700457\n",
      "  (4, 83)\t0.07315626874700457\n",
      "  (4, 85)\t0.07315626874700457\n",
      "  (4, 86)\t0.07315626874700457\n",
      "  (4, 87)\t0.07315626874700457\n",
      "  (4, 88)\t0.07315626874700457\n",
      "  (4, 89)\t0.07315626874700457\n",
      "  (4, 91)\t0.07315626874700457\n",
      "  (4, 93)\t0.07315626874700457\n",
      "  (4, 94)\t0.07315626874700457\n",
      "  (4, 96)\t0.07315626874700457\n",
      "  (4, 97)\t0.07315626874700457\n",
      "  (4, 99)\t0.07315626874700457\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "tfidbow = TFIDFBoW(pipeline)\n",
    "print(tfidbow.fit_transform(train_X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification utilisant BoW\n",
    "\n",
    "Pour la classification, nous allons effectuer une régression logisitique (vu en cours ou que vous allez voir bientôt). \n",
    "Pour en savoir plus : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "La méthode `train_evaluate` entraîne et évalue le modèle de régression logistique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Question 7. Entraînez et calculez la précision de la régression logistique sur les ensembles d'entraînement et de validation. (4 points)\n",
    "\n",
    "Essayez les configurations suivantes :\n",
    "\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "\n",
    "Outre la précision, reportez la taille du dictionnaire pour chacune des configurations. Enfin, décrivez vos résultats obtenus et répondez aux questions suivantes:\n",
    "- Quelles étapes de preprocessing ont effectivement aidé le modèle ? Pourquoi ?\n",
    "- La pondération avec TF-IDF a-t-elle aidé à obtenir une meilleure performance que le simple BoW ?\n",
    "- Les bigrams et trigrams ont-ils amélioré la performance ? Expliquez pourquoi.\n",
    "\n",
    "Indiquez quelle est la configuration que vous choisissez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouns\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-6d2f36fc72c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mbowObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountBoW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclassifier\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbowObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"for  model \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"the train accuracy is: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"and the validation accuracy is :\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalidate_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-c325807147ba>\u001b[0m in \u001b[0;36mtrain_evaluate\u001b[1;34m(training_X, training_Y, validation_X, validation_Y, bowObj)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtraining_rep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbowObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtrainAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_rep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    512\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    515\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#elements for configuration first:countbow, second: tokenization, third:twitter preprocessing, fourth:stemming,\n",
    "#fifth:bigram, six:trigram \n",
    "configurations = [[False,False,False,False,False,False],[False,True,False,False,False,False],[True,True,False,True,False,False],\\\n",
    "                 [True,True,True,True,False,False],[True,True,True,True,True,False],[True,True,True,True,True,True]]\n",
    "for configuration in configurations:\n",
    "        pipeline = PreprocessingPipeline(configuration[1], configuration[2], configuration[3])\n",
    "        if configuration[0] == True:\n",
    "            bowObj = TFIDFBoW(pipeline, configuration[4], configuration[5])\n",
    "        else:\n",
    "            bowObj = CountBoW(pipeline, configuration[4], configuration[5])\n",
    "\n",
    "        classifier , train_acc, validate_acc = train_evaluate(train_X,train_Y,valid_X,valid_Y,bowObj)\n",
    "        print (\"for  model \" + configurations.index(configuration) + \"the train accuracy is: \" + train_acc + \"and the validation accuracy is :\" + validate_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Prototype (7 points)\n",
    "\n",
    "Maintenant que nous avons un modèle de classification entraîné pour l'analyse de sentiments, nous pouvons l'appliquer à notre ensemble de tests et analyser le résultat.\n",
    "\n",
    "## 1. Analyse de Sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 9. Implémentez la fonction `detect_airline` qui détecte la compagnie aérienne d'un tweet. (1,5 points)\n",
    "\n",
    "Expliquez votre approche, et les inconvénients possibles.\n",
    "\n",
    "**Attention :** `detect_airline` doit être en mesure de gérer le cas où aucune compagnie n'est mentionnée (auquel cas `None` est retounée), mais aussi le cas où plusieurs compagnies sont mentionnées dans un tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet):\n",
    "    companies_term = [\"Virgin America\",\"United Airline\",\"Southwest Airlines\", \"jetBlue\", \"USAirways\", \"American Airlines\"]\n",
    "    mentionned_companies = []\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    for term in companies_term:\n",
    "        if term in tweet:\n",
    "            mentionned_companies.append(term)\n",
    "    return mentionned_companies\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Question 10. Implémentez la fonction `extract_sentiment` qui, à partir de tweets et d'un classificateur, extrait leurs sentiments. (0.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweets):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 11. En utilisant `extract_tweet_content`, `detect_airline` et `extract_sentiment`, générez un diagramme en bar contenant le nombre de tweets positives, négatifs et neutres pour chacune des compagnies. (2 points)\n",
    "\n",
    "Décrivez brièvement le diagramme et analysez les résultats (par exemple, quelle est la compagnie avec le plus de tweets négatifs?). Expliquez comment un tel diagramme peut aider des compagnies aériennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse de termes\n",
    "\n",
    "Le POS-tagging (pour *part-of-speech tagging*, en français étiquetage grammatical) consiste à l'extraction de l'information grammaticale d'un token dans une phrase. Par exemple, la table ci-dessous donne un exemple du *POS-tagging* de la phrase *\"The cat is white!\"*\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "Pour autant, le *POS-tagging* peut être plus complexe que les règles simples apprises à l'école. Il faut souvent des informations plus détaillées sur le rôle d'un terme dans une phrase. Pour notre problème, nous n'avons pas besoin d'utiliser un modèle linguistique plus complexe, nous allons utiliser ce qu'on appelle des *POS-tags* universelles.\n",
    "\n",
    "En *POS-tagging*, chaque token est représenté par un tag. La liste des POS-tags utilisés sont disponibles ici :\n",
    "https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 12. Implémentez un code qui collecte les 10 termes les plus fréquents pour chaque compagnie aérienne. (2 Pts)\n",
    "\n",
    "Ici, vous n'allez considérer que les termes apparaissant dans les tweets positifs et négatifs. \n",
    "\n",
    "De plus, nous allons utiliser la définition suivante de \"terme\":\n",
    "\n",
    "1. Un mot qui est soit un adjectif, soit un nom\n",
    "2. Un N-gram composé d'adjectifs suivit par un nom (par exemple, \"nice place\"), ou un nom suivi par un autre nom (par exemple, \"sports club\").\n",
    "\n",
    "Ensuite, **générez une table** contenant les 10 termes les plus fréquents, avec leurs fréquences (en pourcentage) pour chaque compagnie.\n",
    "\n",
    "*N'oubliez pas de supprimer le nom de la compagnie parmi les termes !*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 13. Que conclure de la table généré à la question 12 pour les compagnies ? (1 Pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/ Bonus (2 points)\n",
    "\n",
    "Les noms de personnes, les noms de sociétés et les emplacements sont appelés \"entités nommées\". La reconnaissance d'entité nommée (NER, pour *Named-entity recognition*) consiste à extraire les entités nommées en les classant à l'aide de catégories prédéfinies. Dans cette section bonus, vous utiliserez un outil de NER pour extraire automatiquement des entités nommées des tweets. Cette approche est suffisamment générique pour récupérer des informations sur d’autres sociétés ou même des noms de produits et de personnes.\n",
    "\n",
    "\n",
    "**Pour le bonus, vous êtes libres d'utiliser n'importe quel NER implémenté en Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Bonus 1.  Implémentez un code qui génère une table contenant le top 10 des NER de la base de données. (1 point)\n",
    "\n",
    "Cette table doit contenir les fréquences des entités nommées. Ensuite, générez un diagramme en bar qui montre le nombre de tweets positifs, négatifs ou neutres pour chacunes des 10 NER. Décrivez le résultat obtenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Bonus 2. Générez une table similaire à la question 12 pour le top 10 des NER pour chaque compagnie. (1 point)\n",
    "\n",
    "Que peut-on conclure de ces résultats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
